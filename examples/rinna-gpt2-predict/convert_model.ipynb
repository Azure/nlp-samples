{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# GPT-2 モデルの ONNX 変換と量子化"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 準備\n",
        "\n",
        "Python 3.8 カーネルが存在することが前提\n",
        "\n",
        "```console\n",
        "conda create -n rinna_gpt2_predict python=3.8\n",
        "conda activate rinna_gpt2_predict\n",
        "conda install jupyter\n",
        "jupyter notebook\n",
        "```"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# ライブラリインストール\n",
        "import sys\n",
        "if sys.platform == 'darwin': # Mac\n",
        "    !{sys.executable} -m pip install --upgrade torch torchvision\n",
        "else:\n",
        "    !{sys.executable} -m pip install --upgrade torch==1.9.0+cpu torchvision==0.10.0+cpu torchaudio==0.9.0 -f https://download.pytorch.org/whl/torch_stable.html\n",
        "!{sys.executable} -m pip install onnxruntime==1.8.1\n",
        "!{sys.executable} -m pip install sentencepiece\n",
        "!{sys.executable} -m pip install transformers==4.8.2\n",
        "!{sys.executable} -m pip install onnx onnxconverter_common psutil pytz pandas py-cpuinfo py3nvml sympy coloredlogs azureml-core azureml-mlflow mlflow"
      ],
      "outputs": [],
      "metadata": {
        "gather": {
          "logged": 1628068194057
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# キャッシュ保存用のディレクトリを用意\n",
        "import os\n",
        "cache_dir = os.path.join(\".\", \"cache_models\")\n",
        "if not os.path.exists(cache_dir):\n",
        "    os.makedirs(cache_dir)"
      ],
      "outputs": [],
      "metadata": {
        "gather": {
          "logged": 1628068194169
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# Azure ML Workspace への接続\n",
        "from azureml.core import Experiment, Workspace, Environment\n",
        "from azureml.core.compute import ComputeTarget\n",
        "from azureml.core import ScriptRunConfig\n",
        "from azureml.core.runconfig import PyTorchConfiguration\n",
        "from azureml.core.authentication import InteractiveLoginAuthentication\n",
        "import mlflow\n",
        "\n",
        "interactive_auth = InteractiveLoginAuthentication(force=True,tenant_id=\"72f988bf-86f1-41af-91ab-2d7cd011db47\")\n",
        "ws = Workspace.from_config(path='config.json',auth=interactive_auth)\n",
        "\n",
        "mlflow.set_tracking_uri(ws.get_mlflow_tracking_uri())"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## モデル読み込み"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "client = mlflow.tracking.MlflowClient()"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "registered_model = client.get_model_version(name='test-model',version=37)\n",
        "client.download_artifacts(registered_model.run_id, 'outputs/models', cache_dir)"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# GPT-2 モデルにビームサーチを組み合わせるヘルパー class で読み込んだ GPT-2 モデルをラップ\n",
        "from onnxruntime.transformers.gpt2_beamsearch_helper import Gpt2BeamSearchHelper, GPT2LMHeadModel_BeamSearchStep\n",
        "from transformers import AutoConfig\n",
        "import torch\n",
        "\n",
        "model_name_or_path = os.path.join(cache_dir, 'outputs/models')\n",
        "config = AutoConfig.from_pretrained(model_name_or_path, cache_dir=model_name_or_path)\n",
        "model = GPT2LMHeadModel_BeamSearchStep.from_pretrained(model_name_or_path, config=config, batch_size=1, beam_size=4, cache_dir=cache_dir)\n",
        "device = torch.device(\"cpu\")\n",
        "model.eval().to(device)"
      ],
      "outputs": [],
      "metadata": {
        "gather": {
          "logged": 1628068248507
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# 推論で使う関数用にモデルの情報を取得\n",
        "num_attention_heads = model.config.n_head\n",
        "hidden_size = model.config.n_embd\n",
        "num_layer = model.config.n_layer"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# tokenizer を読み込み\n",
        "tokenizer = T5Tokenizer.from_pretrained(\"rinna/japanese-gpt2-medium\", cache_dir=cache_dir)\n",
        "tokenizer.padding_side = \"left\"\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "tokenizer.do_lower_case = True"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## PyTorch GPT-2 モデルをビームサーチの1ステップを含む ONNX に変換 "
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# ONNX に変換\n",
        "onnx_model_path = os.path.join(cache_dir, \"rinna_gpt2_beam_step_search.onnx\")\n",
        "\n",
        "if not os.path.exists(onnx_model_path):\n",
        "    Gpt2BeamSearchHelper.export_onnx(model, device, onnx_model_path) # add parameter use_external_data_format=True when model size > 2 GB\n",
        "else:\n",
        "    print(\"GPT-2 ONNX model exists.\")"
      ],
      "outputs": [],
      "metadata": {
        "gather": {
          "logged": 1628068248683
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# 最適化と量子化\n",
        "from onnxruntime.transformers.gpt2_helper import Gpt2Helper, MyGPT2LMHeadModel\n",
        "from onnxruntime.transformers.quantize_helper import QuantizeHelper\n",
        "\n",
        "optimized_model_path = os.path.join(cache_dir, \"rinna_gpt2_beam_step_search_optimized.onnx\")\n",
        "quantized_model_path = os.path.join(cache_dir, \"rinna_gpt2_beam_step_search_optimized_int8.onnx\")\n",
        "\n",
        "if not os.path.exists(optimized_model_path):\n",
        "    Gpt2Helper.optimize_onnx(onnx_model_path, optimized_model_path, False, model.config.num_attention_heads, model.config.hidden_size)\n",
        "else:\n",
        "    print(\"Optimized GPT-2 ONNX model exists.\")\n",
        "\n",
        "if not os.path.exists(quantized_model_path):   \n",
        "    QuantizeHelper.quantize_onnx_model(optimized_model_path, quantized_model_path)\n",
        "else:\n",
        "    print(\"Quantized GPT-2 Int8 ONNX model exists.\")\n"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# 量子化した ONNX をモデルとして登録\n",
        "mlflow.set_experiment('register_onnx')\n",
        "with mlflow.start_run() as run:\n",
        "    remote_model_path = os.path.join('outputs','onnx', \"rinna_gpt2_beam_step_search_optimized_int8.onnx\")\n",
        "    mlflow.log_artifact(quantized_model_path, remote_model_path)\n",
        "    model_uri = \"runs:/{}/\".format(run.info.run_id) + remote_model_path\n",
        "    mlflow.register_model(model_uri, 'rinna-GPT2-quantized-model')"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 推論テスト"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "import onnxruntime\n",
        "import numpy\n",
        "from transformers import T5Tokenizer\n",
        "\n",
        "EXAMPLE_Text = ['私はりんなです。']\n",
        "\n",
        "def get_tokenizer(model_name_or_path, cache_dir):\n",
        "    tokenizer = T5Tokenizer.from_pretrained(model_name_or_path, cache_dir=cache_dir)\n",
        "    tokenizer.padding_side = \"left\"\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "    tokenizer.do_lower_case = True\n",
        "    #okenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
        "    return tokenizer\n",
        "\n",
        "def get_example_inputs(prompt_text=EXAMPLE_Text):    \n",
        "    tokenizer = get_tokenizer('rinna/japanese-gpt2-medium', cache_dir)\n",
        "    encodings_dict = tokenizer.batch_encode_plus(prompt_text, padding=True)\n",
        "\n",
        "    input_ids = torch.tensor(encodings_dict['input_ids'], dtype=torch.int64)\n",
        "    attention_mask = torch.tensor(encodings_dict['attention_mask'], dtype=torch.float32)\n",
        "    position_ids = (attention_mask.long().cumsum(-1) - 1)\n",
        "    position_ids.masked_fill_(position_ids < 0, 0)\n",
        "\n",
        "    #Empty Past State for generating first word\n",
        "    empty_past = []\n",
        "    batch_size = input_ids.size(0)\n",
        "    sequence_length = input_ids.size(1)\n",
        "    past_shape = [2, batch_size, num_attention_heads, 0, hidden_size // num_attention_heads]\n",
        "    for i in range(num_layer):\n",
        "        empty_past.append(torch.empty(past_shape).type(torch.float32).to(device))\n",
        "       \n",
        "    return input_ids, attention_mask, position_ids, empty_past\n",
        "\n",
        "input_ids, attention_mask, position_ids, empty_past = get_example_inputs()\n",
        "beam_select_idx = torch.zeros([1, input_ids.shape[0]]).long()\n",
        "input_log_probs = torch.zeros([input_ids.shape[0], 1])\n",
        "input_unfinished_sents = torch.ones([input_ids.shape[0], 1], dtype=torch.bool)\n",
        "prev_step_scores = torch.zeros([input_ids.shape[0], 1])\n",
        "\n",
        "session = onnxruntime.InferenceSession(onnx_model_path)\n",
        "ort_inputs = {\n",
        "              'input_ids': numpy.ascontiguousarray(input_ids.cpu().numpy()),\n",
        "              'attention_mask' : numpy.ascontiguousarray(attention_mask.cpu().numpy()),\n",
        "              'position_ids': numpy.ascontiguousarray(position_ids.cpu().numpy()),\n",
        "              'beam_select_idx': numpy.ascontiguousarray(beam_select_idx.cpu().numpy()),\n",
        "              'input_log_probs': numpy.ascontiguousarray(input_log_probs.cpu().numpy()),\n",
        "              'input_unfinished_sents': numpy.ascontiguousarray(input_unfinished_sents.cpu().numpy()),\n",
        "              'prev_step_results': numpy.ascontiguousarray(input_ids.cpu().numpy()),\n",
        "              'prev_step_scores': numpy.ascontiguousarray(prev_step_scores.cpu().numpy()),\n",
        "             }\n",
        "for i, past_i in enumerate(empty_past):\n",
        "    ort_inputs[f'past_{i}'] = numpy.ascontiguousarray(past_i.cpu().numpy())\n",
        "#print(ort_inputs)\n",
        "ort_outputs = session.run(None, ort_inputs)\n",
        "#print(ort_outputs)"
      ],
      "outputs": [],
      "metadata": {
        "gather": {
          "logged": 1628068299895
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ONNX Runtime Inference with IO Binding\n",
        "\n",
        "GPU を使用する場合の推論パフォーマンス改善"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "def inference_with_io_binding(session, config, input_ids, position_ids, attention_mask, past, beam_select_idx, input_log_probs, input_unfinished_sents, prev_step_results, prev_step_scores, step, context_len):\n",
        "    output_shapes = Gpt2BeamSearchHelper.get_output_shapes(batch_size=1,\n",
        "                                                           context_len=context_len,\n",
        "                                                           past_sequence_length=past[0].size(3),\n",
        "                                                           sequence_length=input_ids.size(1),\n",
        "                                                           beam_size=4,\n",
        "                                                           step=step,\n",
        "                                                           config=config,\n",
        "                                                           model_class=\"GPT2LMHeadModel_BeamSearchStep\")\n",
        "    output_buffers = Gpt2BeamSearchHelper.get_output_buffers(output_shapes, device)\n",
        "\n",
        "    io_binding = Gpt2BeamSearchHelper.prepare_io_binding(session, input_ids, position_ids, attention_mask, past, output_buffers, output_shapes, beam_select_idx, input_log_probs, input_unfinished_sents, prev_step_results, prev_step_scores)\n",
        "    session.run_with_iobinding(io_binding)\n",
        "\n",
        "    outputs = Gpt2BeamSearchHelper.get_outputs_from_io_binding_buffer(session, output_buffers, output_shapes, return_numpy=False)\n",
        "    return outputs"
      ],
      "outputs": [],
      "metadata": {
        "gather": {
          "logged": 1628068300137
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "input_ids, attention_mask, position_ids, empty_past = get_example_inputs()\n",
        "beam_select_idx = torch.zeros([1, input_ids.shape[0]]).long()\n",
        "input_log_probs = torch.zeros([input_ids.shape[0], 1])\n",
        "input_unfinished_sents = torch.ones([input_ids.shape[0], 1], dtype=torch.bool)\n",
        "prev_step_scores = torch.zeros([input_ids.shape[0], 1])\n",
        "outputs = inference_with_io_binding(session, config, input_ids, position_ids, attention_mask, empty_past, beam_select_idx, input_log_probs, input_unfinished_sents, input_ids, prev_step_scores, 0, input_ids.shape[-1])\n",
        "assert torch.eq(outputs[-2], torch.from_numpy(ort_outputs[-2])).all()\n",
        "print(\"IO Binding result is good\")"
      ],
      "outputs": [],
      "metadata": {
        "gather": {
          "logged": 1628068303048
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### バッチ推論"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "def update(output, step, batch_size, beam_size, context_length, prev_attention_mask, device):\n",
        "    \"\"\"\n",
        "    Update the inputs for next inference.\n",
        "    \"\"\"\n",
        "    last_state = (torch.from_numpy(output[0]).to(device)\n",
        "                        if isinstance(output[0], numpy.ndarray) else output[0].clone().detach().cpu())\n",
        "\n",
        "    input_ids = last_state.view(batch_size * beam_size, -1).to(device)\n",
        "\n",
        "    input_unfinished_sents_id = -3\n",
        "    prev_step_results = (torch.from_numpy(output[-2]).to(device) if isinstance(output[-2], numpy.ndarray)\n",
        "                                else output[-2].clone().detach().to(device))\n",
        "    position_ids = (torch.tensor([context_length + step - 1\n",
        "                                        ]).unsqueeze(0).repeat(batch_size * beam_size, 1).to(device))\n",
        "\n",
        "    if prev_attention_mask.shape[0] != (batch_size * beam_size):\n",
        "        prev_attention_mask = prev_attention_mask.repeat(batch_size * beam_size, 1)\n",
        "    attention_mask = torch.cat(\n",
        "        [\n",
        "            prev_attention_mask,\n",
        "            torch.ones([batch_size * beam_size, 1]).type_as(prev_attention_mask),\n",
        "        ],\n",
        "        1,\n",
        "    ).to(device)\n",
        "\n",
        "    beam_select_idx = (torch.from_numpy(output[input_unfinished_sents_id - 2]).to(device) if isinstance(\n",
        "        output[input_unfinished_sents_id - 2], numpy.ndarray) else output[input_unfinished_sents_id - 2].clone().detach().to(device))\n",
        "    input_log_probs = (torch.from_numpy(output[input_unfinished_sents_id - 1]).to(device) if isinstance(\n",
        "        output[input_unfinished_sents_id - 1], numpy.ndarray) else output[input_unfinished_sents_id - 1].clone().detach().to(device))\n",
        "    input_unfinished_sents = (torch.from_numpy(output[input_unfinished_sents_id]).to(device) if isinstance(\n",
        "        output[input_unfinished_sents_id], numpy.ndarray) else\n",
        "                                    output[input_unfinished_sents_id].clone().detach().to(device))\n",
        "    prev_step_scores = (torch.from_numpy(output[-1]).to(device)\n",
        "                                if isinstance(output[-1], numpy.ndarray) else output[-1].clone().detach().to(device))\n",
        "\n",
        "    past = []\n",
        "    if isinstance(output[1], tuple):  # past in torch output is tuple\n",
        "        past = list(output[1])\n",
        "    else:\n",
        "        for i in range(model.config.n_layer):\n",
        "            past_i = (torch.from_numpy(output[i + 1])\n",
        "                        if isinstance(output[i + 1], numpy.ndarray) else output[i + 1].clone().detach())\n",
        "            past.append(past_i.to(device)) \n",
        "\n",
        "    inputs = {\n",
        "        'input_ids': input_ids,\n",
        "        'attention_mask' : attention_mask,\n",
        "        'position_ids': position_ids,\n",
        "        'beam_select_idx': beam_select_idx,\n",
        "        'input_log_probs': input_log_probs,\n",
        "        'input_unfinished_sents': input_unfinished_sents,\n",
        "        'prev_step_results': prev_step_results,\n",
        "        'prev_step_scores': prev_step_scores,\n",
        "    }\n",
        "    ort_inputs = {\n",
        "        'input_ids': numpy.ascontiguousarray(input_ids.cpu().numpy()),\n",
        "        'attention_mask' : numpy.ascontiguousarray(attention_mask.cpu().numpy()),\n",
        "        'position_ids': numpy.ascontiguousarray(position_ids.cpu().numpy()),\n",
        "        'beam_select_idx': numpy.ascontiguousarray(beam_select_idx.cpu().numpy()),\n",
        "        'input_log_probs': numpy.ascontiguousarray(input_log_probs.cpu().numpy()),\n",
        "        'input_unfinished_sents': numpy.ascontiguousarray(input_unfinished_sents.cpu().numpy()),\n",
        "        'prev_step_results': numpy.ascontiguousarray(prev_step_results.cpu().numpy()),\n",
        "        'prev_step_scores': numpy.ascontiguousarray(prev_step_scores.cpu().numpy()),\n",
        "    }\n",
        "    for i, past_i in enumerate(past):\n",
        "        ort_inputs[f'past_{i}'] = numpy.ascontiguousarray(past_i.cpu().numpy())\n",
        "    \n",
        "    return inputs, ort_inputs, past\n",
        "\n",
        "def test_generation(tokenizer, input_text, use_onnxruntime_io, ort_session = None, num_tokens_to_produce = 30):\n",
        "    print(\"Text generation using\", \"OnnxRuntime with IO binding\" if use_onnxruntime_io else \"OnnxRuntime\", \"...\")    \n",
        "    input_ids, attention_mask, position_ids, past = get_example_inputs(input_text)\n",
        "    beam_select_idx = torch.zeros([1, input_ids.shape[0]]).long()\n",
        "    input_log_probs = torch.zeros([input_ids.shape[0], 1])\n",
        "    input_unfinished_sents = torch.ones([input_ids.shape[0], 1], dtype=torch.bool)\n",
        "    prev_step_scores = torch.zeros([input_ids.shape[0], 1])\n",
        "    inputs = {\n",
        "        'input_ids': input_ids,\n",
        "        'attention_mask' : attention_mask,\n",
        "        'position_ids': position_ids,\n",
        "        'beam_select_idx': beam_select_idx,\n",
        "        'input_log_probs': input_log_probs,\n",
        "        'input_unfinished_sents': input_unfinished_sents,\n",
        "        'prev_step_results': input_ids,\n",
        "        'prev_step_scores': prev_step_scores,\n",
        "    }\n",
        "    ort_inputs = {\n",
        "        'input_ids': numpy.ascontiguousarray(input_ids.cpu().numpy()),\n",
        "        'attention_mask' : numpy.ascontiguousarray(attention_mask.cpu().numpy()),\n",
        "        'position_ids': numpy.ascontiguousarray(position_ids.cpu().numpy()),\n",
        "        'beam_select_idx': numpy.ascontiguousarray(beam_select_idx.cpu().numpy()),\n",
        "        'input_log_probs': numpy.ascontiguousarray(input_log_probs.cpu().numpy()),\n",
        "        'input_unfinished_sents': numpy.ascontiguousarray(input_unfinished_sents.cpu().numpy()),\n",
        "        'prev_step_results': numpy.ascontiguousarray(input_ids.cpu().numpy()),\n",
        "        'prev_step_scores': numpy.ascontiguousarray(prev_step_scores.cpu().numpy()),\n",
        "    }\n",
        "    for i, past_i in enumerate(past):\n",
        "        ort_inputs[f'past_{i}'] = numpy.ascontiguousarray(past_i.cpu().numpy())\n",
        "    batch_size = input_ids.size(0)\n",
        "    beam_size = 4\n",
        "    context_length = input_ids.size(-1)\n",
        "\n",
        "    for step in range(num_tokens_to_produce):\n",
        "        if use_onnxruntime_io:\n",
        "            outputs = inference_with_io_binding(ort_session, config, inputs['input_ids'], inputs['position_ids'], inputs['attention_mask'], past, inputs['beam_select_idx'], inputs['input_log_probs'], inputs['input_unfinished_sents'], inputs['prev_step_results'], inputs['prev_step_scores'], step, context_length)\n",
        "        else:\n",
        "            outputs = ort_session.run(None, ort_inputs) \n",
        "        inputs, ort_inputs, past = update(outputs, step, batch_size, beam_size, context_length, inputs['attention_mask'], device)\n",
        "\n",
        "        if not inputs['input_unfinished_sents'].any():\n",
        "            break\n",
        "\n",
        "    print(\"------------\")\n",
        "    print(tokenizer.decode(inputs['prev_step_results'][0], skip_special_tokens=True))"
      ],
      "outputs": [],
      "metadata": {
        "gather": {
          "logged": 1628068303255
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "import time\n",
        "input_text = EXAMPLE_Text"
      ],
      "outputs": [],
      "metadata": {
        "gather": {
          "logged": 1628068306335
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 通常の推論"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "start = time.time()\n",
        "test_generation(tokenizer, input_text, use_onnxruntime_io=False, ort_session=session)    \n",
        "elapsed_time = time.time() - start\n",
        "print (\"elapsed_time:{0}\".format(elapsed_time) + \"[sec]\")"
      ],
      "outputs": [],
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1628068311176
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### IO binding 有効 (GPU 推論をしてないので意味なし)"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "start = time.time()\n",
        "test_generation(tokenizer, input_text, use_onnxruntime_io=True, ort_session=session)\n",
        "elapsed_time = time.time() - start\n",
        "print (\"elapsed_time:{0}\".format(elapsed_time) + \"[sec]\")"
      ],
      "outputs": [],
      "metadata": {
        "gather": {
          "logged": 1628068316843
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 量子化した軽量 GPT-2"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "session_int8 = onnxruntime.InferenceSession(quantized_model_path)\n",
        "\n",
        "start = time.time()\n",
        "test_generation(tokenizer, input_text, use_onnxruntime_io=False, ort_session=session_int8)\n",
        "elapsed_time = time.time() - start\n",
        "print (\"elapsed_time:{0}\".format(elapsed_time) + \"[sec]\")"
      ],
      "outputs": [],
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1628068340468
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [],
      "outputs": [],
      "metadata": {}
    }
  ],
  "metadata": {
    "orig_nbformat": 2,
    "kernelspec": {
      "name": "rinna_gpt2_predict",
      "display_name": "rinna_gpt2_predict",
      "language": "python"
    },
    "metadata": {
      "interpreter": {
        "hash": "81098997110362167705b61d21e46dda767ff2050d805c22b6ba90fec7e1aa35"
      }
    },
    "kernel_info": {
      "name": "cpu_env"
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    },
    "microsoft": {
      "host": {
        "AzureML": {
          "notebookHasBeenCompleted": true
        }
      }
    },
    "interpreter": {
      "hash": "400a85a5d35626d27c756a250f88338f1168ed84ae0abbe7b1b61c0609847b83"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.11",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}