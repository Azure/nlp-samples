{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Optimizing runtime performance on GPT-2 model inference with ONNXRuntime on CPU\n",
        "\n",
        "In this tutorial, you'll be introduced to how to load a GPT2 model from PyTorch, convert it to ONNX with one step search, and inference it using ONNX Runtime with/without IO Binding. GPT-2 model inference is optimized by compiling one-step beam search into the onnx compute graph, which speeds up the runtime significantly. "
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 準備\n",
        "\n",
        "Python 3.8 のカーネルが environment が存在することを前提としています\n",
        "\n",
        "```console\n",
        "conda create -n rinna_gpt2_predict python=3.8\n",
        "conda activate rinna_gpt2_predict\n",
        "conda install jupyter\n",
        "jupyter notebook\n",
        "```"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "import sys\n",
        "if sys.platform == 'darwin': # Mac\n",
        "    !{sys.executable} -m pip install --upgrade torch torchvision\n",
        "else:\n",
        "    !{sys.executable} -m pip install --upgrade torch==1.9.0+cpu torchvision==0.10.0+cpu torchaudio==0.9.0 -f https://download.pytorch.org/whl/torch_stable.html\n",
        "!{sys.executable} -m pip install onnxruntime==1.8.1\n",
        "!{sys.executable} -m pip install sentencepiece\n",
        "!{sys.executable} -m pip install transformers==4.8.2\n",
        "!{sys.executable} -m pip install onnx onnxconverter_common psutil pytz pandas py-cpuinfo py3nvml sympy coloredlogs azureml-core azureml-mlflow mlflow"
      ],
      "outputs": [],
      "metadata": {
        "gather": {
          "logged": 1628068194057
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "source": [
        "# キャッシュ保存用のディレクトリを用意\n",
        "import os\n",
        "cache_dir = os.path.join(\".\", \"cache_models\")\n",
        "if not os.path.exists(cache_dir):\n",
        "    os.makedirs(cache_dir)"
      ],
      "outputs": [],
      "metadata": {
        "gather": {
          "logged": 1628068194169
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "source": [
        "# それ以外\n",
        "from azureml.core import Experiment, Workspace, Environment\n",
        "from azureml.core.compute import ComputeTarget\n",
        "from azureml.core import ScriptRunConfig\n",
        "from azureml.core.runconfig import PyTorchConfiguration\n",
        "from azureml.core.authentication import InteractiveLoginAuthentication\n",
        "import mlflow\n",
        "\n",
        "interactive_auth = InteractiveLoginAuthentication(force=True,tenant_id=\"72f988bf-86f1-41af-91ab-2d7cd011db47\")\n",
        "\n",
        "ws = Workspace.from_config(path='config.json',auth=interactive_auth)\n",
        "print(ws)\n",
        "mlflow.set_tracking_uri(ws.get_mlflow_tracking_uri())"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Performing interactive authentication. Please follow the instructions on the terminal.\n",
            "To sign in, use a web browser to open the page https://microsoft.com/devicelogin and enter the code HTA6S423R to authenticate.\n",
            "You have logged in. Now let us find all the subscriptions to which you have access...\n",
            "Interactive authentication successfully completed.\n",
            "Workspace.create(name='azureml-mlops', subscription_id='82a5d8d3-5322-4c49-b9d6-da6e00be5d57', resource_group='mlops')\n"
          ]
        }
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## PyTorch GPT-2 モデルをビームサーチの1ステップを含む ONNX に変換 "
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "source": [
        "import mlflow\n",
        "import mlflow.pytorch\n",
        "\n",
        "client = mlflow.tracking.MlflowClient()\n",
        "\n",
        "latest_model = client.get_latest_versions(name='test-model') # TODO: 名前指定したい\n",
        "print(latest_model)\n",
        "model_uri=\"runs:/{}/output/\".format(latest_model[0].run_id)\n",
        "model = mlflow.pytorch.load_model(model_uri)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[<ModelVersion: creation_timestamp=1629174236732, current_stage='None', description='', last_updated_timestamp=1629174236732, name='test-model', run_id='414d839f-dd2a-4031-ae36-9c18be36ccce', run_link='', source='azureml://experiments/yaml-rinna-gpt2/runs/414d839f-dd2a-4031-ae36-9c18be36ccce/artifacts/outputs/models', status='READY', status_message='', tags={}, user_id='', version='35'>]\n",
            "runs:/414d839f-dd2a-4031-ae36-9c18be36ccce/output/\n"
          ]
        }
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "from onnxruntime.transformers.gpt2_beamsearch_helper import Gpt2BeamSearchHelper, GPT2LMHeadModel_BeamSearchStep\n",
        "from transformers import AutoConfig\n",
        "import torch\n",
        "\n",
        "model_name_or_path = \"rinna/japanese-gpt2-medium\"\n",
        "config = AutoConfig.from_pretrained(model_name_or_path, cache_dir=cache_dir)\n",
        "model = GPT2LMHeadModel_BeamSearchStep.from_pretrained(model_name_or_path, config=config, batch_size=1, beam_size=4, cache_dir=cache_dir)\n",
        "device = torch.device(\"cpu\")\n",
        "model.eval().to(device)\n",
        "\n",
        "print(model.config)\n",
        "\n",
        "num_attention_heads = model.config.n_head\n",
        "hidden_size = model.config.n_embd\n",
        "num_layer = model.config.n_layer"
      ],
      "outputs": [],
      "metadata": {
        "gather": {
          "logged": 1628068248507
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "onnx_model_path = \"rinna_gpt2_beam_step_search.onnx\"\n",
        "\n",
        "if not os.path.exists(onnx_model_path):\n",
        "    Gpt2BeamSearchHelper.export_onnx(model, device, onnx_model_path) # add parameter use_external_data_format=True when model size > 2 GB\n",
        "else:\n",
        "    print(\"GPT-2 ONNX model exists.\")\n",
        "\n",
        "\n",
        "from onnxruntime.transformers.gpt2_helper import Gpt2Helper, MyGPT2LMHeadModel\n",
        "from onnxruntime.transformers.quantize_helper import QuantizeHelper\n",
        "\n",
        "optimized_fp32_model_path = \"rinna_gpt2_beam_step_search_optimized_fp32.onnx\"\n",
        "quantized_int8_model_path = \"rinna_gpt2_beam_step_search_optimized_gpt2_int8.onnx\"\n",
        "\n",
        "if not os.path.exists(optimized_fp32_model_path):\n",
        "    Gpt2Helper.optimize_onnx(onnx_model_path, optimized_fp32_model_path, False, model.config.num_attention_heads, model.config.hidden_size)\n",
        "else:\n",
        "    print(\"Optimized GPT-2 FP32 ONNX model exists.\")\n",
        "\n",
        "if not os.path.exists(quantized_int8_model_path):   \n",
        "    QuantizeHelper.quantize_onnx_model(optimized_fp32_model_path, quantized_int8_model_path)\n",
        "else:\n",
        "    print(\"Quantized GPT-2 Int8 ONNX model exists.\")\n"
      ],
      "outputs": [],
      "metadata": {
        "gather": {
          "logged": 1628068248683
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ONNX Runtime Inference ##\n",
        "\n",
        "We can use ONNX Runtime to inference. The inputs are dictionary with name and numpy array as value, and the output is list of numpy array. Note that both input and output are in CPU. When you run the inference in GPU, it will involve data copy between CPU and GPU for input and output.\n",
        "\n",
        "Let's create an inference session for ONNX Runtime given the exported ONNX model, and see the output."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "import onnxruntime\n",
        "import numpy\n",
        "from transformers import T5Tokenizer\n",
        "\n",
        "EXAMPLE_Text = ['私はりんなです。']\n",
        "\n",
        "def get_tokenizer(model_name_or_path, cache_dir):\n",
        "    tokenizer = T5Tokenizer.from_pretrained(model_name_or_path, cache_dir=cache_dir)\n",
        "    tokenizer.padding_side = \"left\"\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "    tokenizer.do_lower_case = True\n",
        "    #okenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
        "    return tokenizer\n",
        "\n",
        "def get_example_inputs(prompt_text=EXAMPLE_Text):    \n",
        "    tokenizer = get_tokenizer('rinna/japanese-gpt2-medium', cache_dir)\n",
        "    encodings_dict = tokenizer.batch_encode_plus(prompt_text, padding=True)\n",
        "\n",
        "    input_ids = torch.tensor(encodings_dict['input_ids'], dtype=torch.int64)\n",
        "    attention_mask = torch.tensor(encodings_dict['attention_mask'], dtype=torch.float32)\n",
        "    position_ids = (attention_mask.long().cumsum(-1) - 1)\n",
        "    position_ids.masked_fill_(position_ids < 0, 0)\n",
        "\n",
        "    #Empty Past State for generating first word\n",
        "    empty_past = []\n",
        "    batch_size = input_ids.size(0)\n",
        "    sequence_length = input_ids.size(1)\n",
        "    past_shape = [2, batch_size, num_attention_heads, 0, hidden_size // num_attention_heads]\n",
        "    for i in range(num_layer):\n",
        "        empty_past.append(torch.empty(past_shape).type(torch.float32).to(device))\n",
        "       \n",
        "    return input_ids, attention_mask, position_ids, empty_past\n",
        "\n",
        "input_ids, attention_mask, position_ids, empty_past = get_example_inputs()\n",
        "beam_select_idx = torch.zeros([1, input_ids.shape[0]]).long()\n",
        "input_log_probs = torch.zeros([input_ids.shape[0], 1])\n",
        "input_unfinished_sents = torch.ones([input_ids.shape[0], 1], dtype=torch.bool)\n",
        "prev_step_scores = torch.zeros([input_ids.shape[0], 1])\n",
        "\n",
        "session = onnxruntime.InferenceSession(onnx_model_path)\n",
        "ort_inputs = {\n",
        "              'input_ids': numpy.ascontiguousarray(input_ids.cpu().numpy()),\n",
        "              'attention_mask' : numpy.ascontiguousarray(attention_mask.cpu().numpy()),\n",
        "              'position_ids': numpy.ascontiguousarray(position_ids.cpu().numpy()),\n",
        "              'beam_select_idx': numpy.ascontiguousarray(beam_select_idx.cpu().numpy()),\n",
        "              'input_log_probs': numpy.ascontiguousarray(input_log_probs.cpu().numpy()),\n",
        "              'input_unfinished_sents': numpy.ascontiguousarray(input_unfinished_sents.cpu().numpy()),\n",
        "              'prev_step_results': numpy.ascontiguousarray(input_ids.cpu().numpy()),\n",
        "              'prev_step_scores': numpy.ascontiguousarray(prev_step_scores.cpu().numpy()),\n",
        "             }\n",
        "for i, past_i in enumerate(empty_past):\n",
        "    ort_inputs[f'past_{i}'] = numpy.ascontiguousarray(past_i.cpu().numpy())\n",
        "#print(ort_inputs)\n",
        "ort_outputs = session.run(None, ort_inputs)\n",
        "#print(ort_outputs)"
      ],
      "outputs": [],
      "metadata": {
        "gather": {
          "logged": 1628068299895
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ONNX Runtime Inference with IO Binding ##\n",
        "\n",
        "To avoid data copy for input and output, ONNX Runtime also supports IO Binding. User could provide some buffer for input and outputs. For GPU inference, the buffer can be in GPU to reduce memory copy between CPU and GPU. This is helpful for high performance inference in GPU. For GPT-2, IO Binding might help the performance when batch size or (past) sequence length is large."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "def inference_with_io_binding(session, config, input_ids, position_ids, attention_mask, past, beam_select_idx, input_log_probs, input_unfinished_sents, prev_step_results, prev_step_scores, step, context_len):\n",
        "    output_shapes = Gpt2BeamSearchHelper.get_output_shapes(batch_size=1,\n",
        "                                                           context_len=context_len,\n",
        "                                                           past_sequence_length=past[0].size(3),\n",
        "                                                           sequence_length=input_ids.size(1),\n",
        "                                                           beam_size=4,\n",
        "                                                           step=step,\n",
        "                                                           config=config,\n",
        "                                                           model_class=\"GPT2LMHeadModel_BeamSearchStep\")\n",
        "    output_buffers = Gpt2BeamSearchHelper.get_output_buffers(output_shapes, device)\n",
        "\n",
        "    io_binding = Gpt2BeamSearchHelper.prepare_io_binding(session, input_ids, position_ids, attention_mask, past, output_buffers, output_shapes, beam_select_idx, input_log_probs, input_unfinished_sents, prev_step_results, prev_step_scores)\n",
        "    session.run_with_iobinding(io_binding)\n",
        "\n",
        "    outputs = Gpt2BeamSearchHelper.get_outputs_from_io_binding_buffer(session, output_buffers, output_shapes, return_numpy=False)\n",
        "    return outputs"
      ],
      "outputs": [],
      "metadata": {
        "gather": {
          "logged": 1628068300137
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can see that the result is exactly same with/without IO Binding:"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "input_ids, attention_mask, position_ids, empty_past = get_example_inputs()\n",
        "beam_select_idx = torch.zeros([1, input_ids.shape[0]]).long()\n",
        "input_log_probs = torch.zeros([input_ids.shape[0], 1])\n",
        "input_unfinished_sents = torch.ones([input_ids.shape[0], 1], dtype=torch.bool)\n",
        "prev_step_scores = torch.zeros([input_ids.shape[0], 1])\n",
        "outputs = inference_with_io_binding(session, config, input_ids, position_ids, attention_mask, empty_past, beam_select_idx, input_log_probs, input_unfinished_sents, input_ids, prev_step_scores, 0, input_ids.shape[-1])\n",
        "assert torch.eq(outputs[-2], torch.from_numpy(ort_outputs[-2])).all()\n",
        "print(\"IO Binding result is good\")"
      ],
      "outputs": [],
      "metadata": {
        "gather": {
          "logged": 1628068303048
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Batch Text Generation ##\n",
        "\n",
        "Here is an example for text generation using ONNX Runtime with/without IO Binding."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "def update(output, step, batch_size, beam_size, context_length, prev_attention_mask, device):\n",
        "    \"\"\"\n",
        "    Update the inputs for next inference.\n",
        "    \"\"\"\n",
        "    last_state = (torch.from_numpy(output[0]).to(device)\n",
        "                        if isinstance(output[0], numpy.ndarray) else output[0].clone().detach().cpu())\n",
        "\n",
        "    input_ids = last_state.view(batch_size * beam_size, -1).to(device)\n",
        "\n",
        "    input_unfinished_sents_id = -3\n",
        "    prev_step_results = (torch.from_numpy(output[-2]).to(device) if isinstance(output[-2], numpy.ndarray)\n",
        "                                else output[-2].clone().detach().to(device))\n",
        "    position_ids = (torch.tensor([context_length + step - 1\n",
        "                                        ]).unsqueeze(0).repeat(batch_size * beam_size, 1).to(device))\n",
        "\n",
        "    if prev_attention_mask.shape[0] != (batch_size * beam_size):\n",
        "        prev_attention_mask = prev_attention_mask.repeat(batch_size * beam_size, 1)\n",
        "    attention_mask = torch.cat(\n",
        "        [\n",
        "            prev_attention_mask,\n",
        "            torch.ones([batch_size * beam_size, 1]).type_as(prev_attention_mask),\n",
        "        ],\n",
        "        1,\n",
        "    ).to(device)\n",
        "\n",
        "    beam_select_idx = (torch.from_numpy(output[input_unfinished_sents_id - 2]).to(device) if isinstance(\n",
        "        output[input_unfinished_sents_id - 2], numpy.ndarray) else output[input_unfinished_sents_id - 2].clone().detach().to(device))\n",
        "    input_log_probs = (torch.from_numpy(output[input_unfinished_sents_id - 1]).to(device) if isinstance(\n",
        "        output[input_unfinished_sents_id - 1], numpy.ndarray) else output[input_unfinished_sents_id - 1].clone().detach().to(device))\n",
        "    input_unfinished_sents = (torch.from_numpy(output[input_unfinished_sents_id]).to(device) if isinstance(\n",
        "        output[input_unfinished_sents_id], numpy.ndarray) else\n",
        "                                    output[input_unfinished_sents_id].clone().detach().to(device))\n",
        "    prev_step_scores = (torch.from_numpy(output[-1]).to(device)\n",
        "                                if isinstance(output[-1], numpy.ndarray) else output[-1].clone().detach().to(device))\n",
        "\n",
        "    past = []\n",
        "    if isinstance(output[1], tuple):  # past in torch output is tuple\n",
        "        past = list(output[1])\n",
        "    else:\n",
        "        for i in range(model.config.n_layer):\n",
        "            past_i = (torch.from_numpy(output[i + 1])\n",
        "                        if isinstance(output[i + 1], numpy.ndarray) else output[i + 1].clone().detach())\n",
        "            past.append(past_i.to(device)) \n",
        "\n",
        "    inputs = {\n",
        "        'input_ids': input_ids,\n",
        "        'attention_mask' : attention_mask,\n",
        "        'position_ids': position_ids,\n",
        "        'beam_select_idx': beam_select_idx,\n",
        "        'input_log_probs': input_log_probs,\n",
        "        'input_unfinished_sents': input_unfinished_sents,\n",
        "        'prev_step_results': prev_step_results,\n",
        "        'prev_step_scores': prev_step_scores,\n",
        "    }\n",
        "    ort_inputs = {\n",
        "        'input_ids': numpy.ascontiguousarray(input_ids.cpu().numpy()),\n",
        "        'attention_mask' : numpy.ascontiguousarray(attention_mask.cpu().numpy()),\n",
        "        'position_ids': numpy.ascontiguousarray(position_ids.cpu().numpy()),\n",
        "        'beam_select_idx': numpy.ascontiguousarray(beam_select_idx.cpu().numpy()),\n",
        "        'input_log_probs': numpy.ascontiguousarray(input_log_probs.cpu().numpy()),\n",
        "        'input_unfinished_sents': numpy.ascontiguousarray(input_unfinished_sents.cpu().numpy()),\n",
        "        'prev_step_results': numpy.ascontiguousarray(prev_step_results.cpu().numpy()),\n",
        "        'prev_step_scores': numpy.ascontiguousarray(prev_step_scores.cpu().numpy()),\n",
        "    }\n",
        "    for i, past_i in enumerate(past):\n",
        "        ort_inputs[f'past_{i}'] = numpy.ascontiguousarray(past_i.cpu().numpy())\n",
        "    \n",
        "    return inputs, ort_inputs, past\n",
        "\n",
        "def test_generation(tokenizer, input_text, use_onnxruntime_io, ort_session = None, num_tokens_to_produce = 30):\n",
        "    print(\"Text generation using\", \"OnnxRuntime with IO binding\" if use_onnxruntime_io else \"OnnxRuntime\", \"...\")    \n",
        "    input_ids, attention_mask, position_ids, past = get_example_inputs(input_text)\n",
        "    beam_select_idx = torch.zeros([1, input_ids.shape[0]]).long()\n",
        "    input_log_probs = torch.zeros([input_ids.shape[0], 1])\n",
        "    input_unfinished_sents = torch.ones([input_ids.shape[0], 1], dtype=torch.bool)\n",
        "    prev_step_scores = torch.zeros([input_ids.shape[0], 1])\n",
        "    inputs = {\n",
        "        'input_ids': input_ids,\n",
        "        'attention_mask' : attention_mask,\n",
        "        'position_ids': position_ids,\n",
        "        'beam_select_idx': beam_select_idx,\n",
        "        'input_log_probs': input_log_probs,\n",
        "        'input_unfinished_sents': input_unfinished_sents,\n",
        "        'prev_step_results': input_ids,\n",
        "        'prev_step_scores': prev_step_scores,\n",
        "    }\n",
        "    ort_inputs = {\n",
        "        'input_ids': numpy.ascontiguousarray(input_ids.cpu().numpy()),\n",
        "        'attention_mask' : numpy.ascontiguousarray(attention_mask.cpu().numpy()),\n",
        "        'position_ids': numpy.ascontiguousarray(position_ids.cpu().numpy()),\n",
        "        'beam_select_idx': numpy.ascontiguousarray(beam_select_idx.cpu().numpy()),\n",
        "        'input_log_probs': numpy.ascontiguousarray(input_log_probs.cpu().numpy()),\n",
        "        'input_unfinished_sents': numpy.ascontiguousarray(input_unfinished_sents.cpu().numpy()),\n",
        "        'prev_step_results': numpy.ascontiguousarray(input_ids.cpu().numpy()),\n",
        "        'prev_step_scores': numpy.ascontiguousarray(prev_step_scores.cpu().numpy()),\n",
        "    }\n",
        "    for i, past_i in enumerate(past):\n",
        "        ort_inputs[f'past_{i}'] = numpy.ascontiguousarray(past_i.cpu().numpy())\n",
        "    batch_size = input_ids.size(0)\n",
        "    beam_size = 4\n",
        "    context_length = input_ids.size(-1)\n",
        "\n",
        "    for step in range(num_tokens_to_produce):\n",
        "        if use_onnxruntime_io:\n",
        "            outputs = inference_with_io_binding(ort_session, config, inputs['input_ids'], inputs['position_ids'], inputs['attention_mask'], past, inputs['beam_select_idx'], inputs['input_log_probs'], inputs['input_unfinished_sents'], inputs['prev_step_results'], inputs['prev_step_scores'], step, context_length)\n",
        "        else:\n",
        "            outputs = ort_session.run(None, ort_inputs) \n",
        "        inputs, ort_inputs, past = update(outputs, step, batch_size, beam_size, context_length, inputs['attention_mask'], device)\n",
        "\n",
        "        if not inputs['input_unfinished_sents'].any():\n",
        "            break\n",
        "\n",
        "    print(\"------------\")\n",
        "    print(tokenizer.decode(inputs['prev_step_results'][0], skip_special_tokens=True))"
      ],
      "outputs": [],
      "metadata": {
        "gather": {
          "logged": 1628068303255
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "import time\n",
        "tokenizer = get_tokenizer(model_name_or_path, cache_dir)\n",
        "input_text = EXAMPLE_Text"
      ],
      "outputs": [],
      "metadata": {
        "gather": {
          "logged": 1628068306335
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "start = time.time()\n",
        "test_generation(tokenizer, input_text, use_onnxruntime_io=False, ort_session=session)    \n",
        "elapsed_time = time.time() - start\n",
        "print (\"elapsed_time:{0}\".format(elapsed_time) + \"[sec]\")"
      ],
      "outputs": [],
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1628068311176
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, we use ONNX Runtime with IO binding to run again and we can see that the result is exactly same."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "start = time.time()\n",
        "test_generation(tokenizer, input_text, use_onnxruntime_io=True, ort_session=session)\n",
        "elapsed_time = time.time() - start\n",
        "print (\"elapsed_time:{0}\".format(elapsed_time) + \"[sec]\")"
      ],
      "outputs": [],
      "metadata": {
        "gather": {
          "logged": 1628068316843
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Quantatize GPT-2"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "session_int8 = onnxruntime.InferenceSession(quantized_int8_model_path)\n",
        "\n",
        "start = time.time()\n",
        "test_generation(tokenizer, input_text, use_onnxruntime_io=False, ort_session=session_int8)\n",
        "elapsed_time = time.time() - start\n",
        "print (\"elapsed_time:{0}\".format(elapsed_time) + \"[sec]\")"
      ],
      "outputs": [],
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1628068340468
        }
      }
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python",
      "version": "3.8.10",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "orig_nbformat": 2,
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3.8.10 64-bit ('py38-mlflow': conda)"
    },
    "metadata": {
      "interpreter": {
        "hash": "81098997110362167705b61d21e46dda767ff2050d805c22b6ba90fec7e1aa35"
      }
    },
    "kernel_info": {
      "name": "cpu_env"
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    },
    "microsoft": {
      "host": {
        "AzureML": {
          "notebookHasBeenCompleted": true
        }
      }
    },
    "interpreter": {
      "hash": "400a85a5d35626d27c756a250f88338f1168ed84ae0abbe7b1b61c0609847b83"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}