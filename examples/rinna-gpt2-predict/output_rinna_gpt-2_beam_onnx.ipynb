{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# GPT-2 モデルの ONNX 変換と量子化"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 準備\n",
        "\n",
        "Python 3.8 のカーネルが存在することを前提としています\n",
        "\n",
        "```console\n",
        "conda create -n rinna_gpt2_predict python=3.8\n",
        "conda activate rinna_gpt2_predict\n",
        "conda install jupyter\n",
        "jupyter notebook\n",
        "```"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "source": [
        "# ライブラリインストール\n",
        "import sys\n",
        "if sys.platform == 'darwin': # Mac\n",
        "    !{sys.executable} -m pip install --upgrade torch torchvision\n",
        "else:\n",
        "    !{sys.executable} -m pip install --upgrade torch==1.9.0+cpu torchvision==0.10.0+cpu torchaudio==0.9.0 -f https://download.pytorch.org/whl/torch_stable.html\n",
        "!{sys.executable} -m pip install onnxruntime==1.8.1\n",
        "!{sys.executable} -m pip install sentencepiece\n",
        "!{sys.executable} -m pip install transformers==4.8.2\n",
        "!{sys.executable} -m pip install onnx onnxconverter_common psutil pytz pandas py-cpuinfo py3nvml sympy coloredlogs azureml-core azureml-mlflow mlflow"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in links: https://download.pytorch.org/whl/torch_stable.html\n",
            "Requirement already satisfied: torch==1.9.0+cpu in /anaconda/envs/rinna_gpt2_predict/lib/python3.8/site-packages (1.9.0+cpu)\n",
            "Requirement already satisfied: torchvision==0.10.0+cpu in /anaconda/envs/rinna_gpt2_predict/lib/python3.8/site-packages (0.10.0+cpu)\n",
            "Requirement already satisfied: torchaudio==0.9.0 in /anaconda/envs/rinna_gpt2_predict/lib/python3.8/site-packages (0.9.0)\n",
            "Requirement already satisfied: typing-extensions in /anaconda/envs/rinna_gpt2_predict/lib/python3.8/site-packages (from torch==1.9.0+cpu) (3.10.0.0)\n",
            "Requirement already satisfied: pillow>=5.3.0 in /anaconda/envs/rinna_gpt2_predict/lib/python3.8/site-packages (from torchvision==0.10.0+cpu) (8.3.1)\n",
            "Requirement already satisfied: numpy in /anaconda/envs/rinna_gpt2_predict/lib/python3.8/site-packages (from torchvision==0.10.0+cpu) (1.21.2)\n",
            "Requirement already satisfied: onnxruntime==1.8.1 in /anaconda/envs/rinna_gpt2_predict/lib/python3.8/site-packages (1.8.1)\n",
            "Requirement already satisfied: protobuf in /anaconda/envs/rinna_gpt2_predict/lib/python3.8/site-packages (from onnxruntime==1.8.1) (3.17.3)\n",
            "Requirement already satisfied: numpy>=1.16.6 in /anaconda/envs/rinna_gpt2_predict/lib/python3.8/site-packages (from onnxruntime==1.8.1) (1.21.2)\n",
            "Requirement already satisfied: flatbuffers in /anaconda/envs/rinna_gpt2_predict/lib/python3.8/site-packages (from onnxruntime==1.8.1) (2.0)\n",
            "Requirement already satisfied: six>=1.9 in /anaconda/envs/rinna_gpt2_predict/lib/python3.8/site-packages (from protobuf->onnxruntime==1.8.1) (1.16.0)\n",
            "Requirement already satisfied: sentencepiece in /anaconda/envs/rinna_gpt2_predict/lib/python3.8/site-packages (0.1.96)\n",
            "Requirement already satisfied: transformers==4.8.2 in /anaconda/envs/rinna_gpt2_predict/lib/python3.8/site-packages (4.8.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /anaconda/envs/rinna_gpt2_predict/lib/python3.8/site-packages (from transformers==4.8.2) (1.21.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /anaconda/envs/rinna_gpt2_predict/lib/python3.8/site-packages (from transformers==4.8.2) (2021.8.3)\n",
            "Requirement already satisfied: requests in /anaconda/envs/rinna_gpt2_predict/lib/python3.8/site-packages (from transformers==4.8.2) (2.26.0)\n",
            "Requirement already satisfied: huggingface-hub==0.0.12 in /anaconda/envs/rinna_gpt2_predict/lib/python3.8/site-packages (from transformers==4.8.2) (0.0.12)\n",
            "Requirement already satisfied: pyyaml in /anaconda/envs/rinna_gpt2_predict/lib/python3.8/site-packages (from transformers==4.8.2) (5.4.1)\n",
            "Requirement already satisfied: filelock in /anaconda/envs/rinna_gpt2_predict/lib/python3.8/site-packages (from transformers==4.8.2) (3.0.12)\n",
            "Requirement already satisfied: sacremoses in /anaconda/envs/rinna_gpt2_predict/lib/python3.8/site-packages (from transformers==4.8.2) (0.0.45)\n",
            "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /anaconda/envs/rinna_gpt2_predict/lib/python3.8/site-packages (from transformers==4.8.2) (0.10.3)\n",
            "Requirement already satisfied: packaging in /anaconda/envs/rinna_gpt2_predict/lib/python3.8/site-packages (from transformers==4.8.2) (21.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /anaconda/envs/rinna_gpt2_predict/lib/python3.8/site-packages (from transformers==4.8.2) (4.62.1)\n",
            "Requirement already satisfied: typing-extensions in /anaconda/envs/rinna_gpt2_predict/lib/python3.8/site-packages (from huggingface-hub==0.0.12->transformers==4.8.2) (3.10.0.0)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /anaconda/envs/rinna_gpt2_predict/lib/python3.8/site-packages (from packaging->transformers==4.8.2) (2.4.7)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /anaconda/envs/rinna_gpt2_predict/lib/python3.8/site-packages (from requests->transformers==4.8.2) (3.2)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /anaconda/envs/rinna_gpt2_predict/lib/python3.8/site-packages (from requests->transformers==4.8.2) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /anaconda/envs/rinna_gpt2_predict/lib/python3.8/site-packages (from requests->transformers==4.8.2) (2021.5.30)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /anaconda/envs/rinna_gpt2_predict/lib/python3.8/site-packages (from requests->transformers==4.8.2) (1.26.5)\n",
            "Requirement already satisfied: six in /anaconda/envs/rinna_gpt2_predict/lib/python3.8/site-packages (from sacremoses->transformers==4.8.2) (1.16.0)\n",
            "Requirement already satisfied: click in /anaconda/envs/rinna_gpt2_predict/lib/python3.8/site-packages (from sacremoses->transformers==4.8.2) (8.0.1)\n",
            "Requirement already satisfied: joblib in /anaconda/envs/rinna_gpt2_predict/lib/python3.8/site-packages (from sacremoses->transformers==4.8.2) (1.0.1)\n",
            "Requirement already satisfied: onnx in /anaconda/envs/rinna_gpt2_predict/lib/python3.8/site-packages (1.10.1)\n",
            "Requirement already satisfied: onnxconverter_common in /anaconda/envs/rinna_gpt2_predict/lib/python3.8/site-packages (1.8.1)\n",
            "Requirement already satisfied: psutil in /anaconda/envs/rinna_gpt2_predict/lib/python3.8/site-packages (5.8.0)\n",
            "Requirement already satisfied: pytz in /anaconda/envs/rinna_gpt2_predict/lib/python3.8/site-packages (2021.1)\n",
            "Requirement already satisfied: pandas in /anaconda/envs/rinna_gpt2_predict/lib/python3.8/site-packages (1.3.2)\n",
            "Requirement already satisfied: py-cpuinfo in /anaconda/envs/rinna_gpt2_predict/lib/python3.8/site-packages (8.0.0)\n",
            "Requirement already satisfied: py3nvml in /anaconda/envs/rinna_gpt2_predict/lib/python3.8/site-packages (0.2.6)\n",
            "Requirement already satisfied: sympy in /anaconda/envs/rinna_gpt2_predict/lib/python3.8/site-packages (1.8)\n",
            "Requirement already satisfied: coloredlogs in /anaconda/envs/rinna_gpt2_predict/lib/python3.8/site-packages (15.0.1)\n",
            "Requirement already satisfied: azureml-core in /anaconda/envs/rinna_gpt2_predict/lib/python3.8/site-packages (1.33.0)\n",
            "Requirement already satisfied: azureml-mlflow in /anaconda/envs/rinna_gpt2_predict/lib/python3.8/site-packages (1.33.0)\n",
            "Requirement already satisfied: mlflow in /anaconda/envs/rinna_gpt2_predict/lib/python3.8/site-packages (1.19.0)\n",
            "Requirement already satisfied: azure-graphrbac<1.0.0,>=0.40.0 in /anaconda/envs/rinna_gpt2_predict/lib/python3.8/site-packages (from azureml-core) (0.61.1)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.19.1 in /anaconda/envs/rinna_gpt2_predict/lib/python3.8/site-packages (from azureml-core) (2.26.0)\n",
            "Requirement already satisfied: azure-mgmt-storage<16.0.0,>=1.5.0 in /anaconda/envs/rinna_gpt2_predict/lib/python3.8/site-packages (from azureml-core) (11.2.0)\n",
            "Requirement already satisfied: cryptography!=1.9,!=2.0.*,!=2.1.*,!=2.2.*,<4.0.0 in /anaconda/envs/rinna_gpt2_predict/lib/python3.8/site-packages (from azureml-core) (3.4.7)\n",
            "Requirement already satisfied: urllib3<=1.26.5,>=1.23 in /anaconda/envs/rinna_gpt2_predict/lib/python3.8/site-packages (from azureml-core) (1.26.5)\n",
            "Requirement already satisfied: pyopenssl<21.0.0 in /anaconda/envs/rinna_gpt2_predict/lib/python3.8/site-packages (from azureml-core) (20.0.1)\n",
            "Requirement already satisfied: azure-mgmt-authorization<1.0.0,>=0.40.0 in /anaconda/envs/rinna_gpt2_predict/lib/python3.8/site-packages (from azureml-core) (0.61.0)\n",
            "Requirement already satisfied: ndg-httpsclient<=0.5.1 in /anaconda/envs/rinna_gpt2_predict/lib/python3.8/site-packages (from azureml-core) (0.5.1)\n",
            "Requirement already satisfied: adal<=1.2.7,>=1.2.0 in /anaconda/envs/rinna_gpt2_predict/lib/python3.8/site-packages (from azureml-core) (1.2.7)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.7.3 in /anaconda/envs/rinna_gpt2_predict/lib/python3.8/site-packages (from azureml-core) (2.8.2)\n",
            "Requirement already satisfied: msrest<1.0.0,>=0.5.1 in /anaconda/envs/rinna_gpt2_predict/lib/python3.8/site-packages (from azureml-core) (0.6.21)\n",
            "Requirement already satisfied: backports.tempfile in /anaconda/envs/rinna_gpt2_predict/lib/python3.8/site-packages (from azureml-core) (1.0)\n",
            "Requirement already satisfied: ruamel.yaml<0.17.5,>=0.15.35 in /anaconda/envs/rinna_gpt2_predict/lib/python3.8/site-packages (from azureml-core) (0.17.4)\n",
            "Requirement already satisfied: jsonpickle<3.0.0 in /anaconda/envs/rinna_gpt2_predict/lib/python3.8/site-packages (from azureml-core) (2.0.0)\n",
            "Requirement already satisfied: pathspec<1.0.0 in /anaconda/envs/rinna_gpt2_predict/lib/python3.8/site-packages (from azureml-core) (0.9.0)\n",
            "Requirement already satisfied: azure-mgmt-keyvault<10.0.0,>=0.40.0 in /anaconda/envs/rinna_gpt2_predict/lib/python3.8/site-packages (from azureml-core) (9.0.0)\n",
            "Requirement already satisfied: contextlib2<1.0.0 in /anaconda/envs/rinna_gpt2_predict/lib/python3.8/site-packages (from azureml-core) (0.6.0.post1)\n",
            "Requirement already satisfied: docker<5.0.0 in /anaconda/envs/rinna_gpt2_predict/lib/python3.8/site-packages (from azureml-core) (4.4.4)\n",
            "Requirement already satisfied: jmespath<1.0.0 in /anaconda/envs/rinna_gpt2_predict/lib/python3.8/site-packages (from azureml-core) (0.10.0)\n",
            "Requirement already satisfied: azure-common<2.0.0,>=1.1.12 in /anaconda/envs/rinna_gpt2_predict/lib/python3.8/site-packages (from azureml-core) (1.1.27)\n",
            "Requirement already satisfied: SecretStorage<4.0.0 in /anaconda/envs/rinna_gpt2_predict/lib/python3.8/site-packages (from azureml-core) (3.3.1)\n",
            "Requirement already satisfied: azure-mgmt-resource<15.0.0,>=1.2.1 in /anaconda/envs/rinna_gpt2_predict/lib/python3.8/site-packages (from azureml-core) (13.0.0)\n",
            "Requirement already satisfied: msrestazure<=0.6.4,>=0.4.33 in /anaconda/envs/rinna_gpt2_predict/lib/python3.8/site-packages (from azureml-core) (0.6.4)\n",
            "Requirement already satisfied: azure-mgmt-containerregistry>=2.0.0 in /anaconda/envs/rinna_gpt2_predict/lib/python3.8/site-packages (from azureml-core) (8.1.0)\n",
            "Requirement already satisfied: PyJWT<3.0.0 in /anaconda/envs/rinna_gpt2_predict/lib/python3.8/site-packages (from azureml-core) (2.1.0)\n",
            "Requirement already satisfied: azure-mgmt-core<2.0.0,>=1.2.0 in /anaconda/envs/rinna_gpt2_predict/lib/python3.8/site-packages (from azure-mgmt-containerregistry>=2.0.0->azureml-core) (1.3.0)\n",
            "Requirement already satisfied: azure-core<2.0.0,>=1.15.0 in /anaconda/envs/rinna_gpt2_predict/lib/python3.8/site-packages (from azure-mgmt-core<2.0.0,>=1.2.0->azure-mgmt-containerregistry>=2.0.0->azureml-core) (1.17.0)\n",
            "Requirement already satisfied: six>=1.11.0 in /anaconda/envs/rinna_gpt2_predict/lib/python3.8/site-packages (from azure-core<2.0.0,>=1.15.0->azure-mgmt-core<2.0.0,>=1.2.0->azure-mgmt-containerregistry>=2.0.0->azureml-core) (1.16.0)\n",
            "Requirement already satisfied: cffi>=1.12 in /anaconda/envs/rinna_gpt2_predict/lib/python3.8/site-packages (from cryptography!=1.9,!=2.0.*,!=2.1.*,!=2.2.*,<4.0.0->azureml-core) (1.14.6)\n",
            "Requirement already satisfied: pycparser in /anaconda/envs/rinna_gpt2_predict/lib/python3.8/site-packages (from cffi>=1.12->cryptography!=1.9,!=2.0.*,!=2.1.*,!=2.2.*,<4.0.0->azureml-core) (2.20)\n",
            "Requirement already satisfied: websocket-client>=0.32.0 in /anaconda/envs/rinna_gpt2_predict/lib/python3.8/site-packages (from docker<5.0.0->azureml-core) (1.2.1)\n",
            "Requirement already satisfied: isodate>=0.6.0 in /anaconda/envs/rinna_gpt2_predict/lib/python3.8/site-packages (from msrest<1.0.0,>=0.5.1->azureml-core) (0.6.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /anaconda/envs/rinna_gpt2_predict/lib/python3.8/site-packages (from msrest<1.0.0,>=0.5.1->azureml-core) (2021.5.30)\n",
            "Requirement already satisfied: requests-oauthlib>=0.5.0 in /anaconda/envs/rinna_gpt2_predict/lib/python3.8/site-packages (from msrest<1.0.0,>=0.5.1->azureml-core) (1.3.0)\n",
            "Requirement already satisfied: pyasn1>=0.1.1 in /anaconda/envs/rinna_gpt2_predict/lib/python3.8/site-packages (from ndg-httpsclient<=0.5.1->azureml-core) (0.4.8)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /anaconda/envs/rinna_gpt2_predict/lib/python3.8/site-packages (from requests<3.0.0,>=2.19.1->azureml-core) (3.2)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /anaconda/envs/rinna_gpt2_predict/lib/python3.8/site-packages (from requests<3.0.0,>=2.19.1->azureml-core) (2.0.4)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /anaconda/envs/rinna_gpt2_predict/lib/python3.8/site-packages (from requests-oauthlib>=0.5.0->msrest<1.0.0,>=0.5.1->azureml-core) (3.1.1)\n",
            "Requirement already satisfied: ruamel.yaml.clib>=0.1.2 in /anaconda/envs/rinna_gpt2_predict/lib/python3.8/site-packages (from ruamel.yaml<0.17.5,>=0.15.35->azureml-core) (0.2.6)\n",
            "Requirement already satisfied: jeepney>=0.6 in /anaconda/envs/rinna_gpt2_predict/lib/python3.8/site-packages (from SecretStorage<4.0.0->azureml-core) (0.7.1)\n",
            "Requirement already satisfied: mlflow-skinny in /anaconda/envs/rinna_gpt2_predict/lib/python3.8/site-packages (from azureml-mlflow) (1.19.0)\n",
            "Requirement already satisfied: humanfriendly>=9.1 in /anaconda/envs/rinna_gpt2_predict/lib/python3.8/site-packages (from coloredlogs) (9.2)\n",
            "Requirement already satisfied: prometheus-flask-exporter in /anaconda/envs/rinna_gpt2_predict/lib/python3.8/site-packages (from mlflow) (0.18.2)\n",
            "Requirement already satisfied: gunicorn in /anaconda/envs/rinna_gpt2_predict/lib/python3.8/site-packages (from mlflow) (20.1.0)\n",
            "Requirement already satisfied: click>=7.0 in /anaconda/envs/rinna_gpt2_predict/lib/python3.8/site-packages (from mlflow) (8.0.1)\n",
            "Requirement already satisfied: Flask in /anaconda/envs/rinna_gpt2_predict/lib/python3.8/site-packages (from mlflow) (2.0.1)\n",
            "Requirement already satisfied: cloudpickle in /anaconda/envs/rinna_gpt2_predict/lib/python3.8/site-packages (from mlflow) (1.6.0)\n",
            "Requirement already satisfied: numpy in /anaconda/envs/rinna_gpt2_predict/lib/python3.8/site-packages (from mlflow) (1.21.2)\n",
            "Requirement already satisfied: gitpython>=2.1.0 in /anaconda/envs/rinna_gpt2_predict/lib/python3.8/site-packages (from mlflow) (3.1.18)\n",
            "Requirement already satisfied: querystring-parser in /anaconda/envs/rinna_gpt2_predict/lib/python3.8/site-packages (from mlflow) (1.2.4)\n",
            "Requirement already satisfied: packaging in /anaconda/envs/rinna_gpt2_predict/lib/python3.8/site-packages (from mlflow) (21.0)\n",
            "Requirement already satisfied: databricks-cli>=0.8.7 in /anaconda/envs/rinna_gpt2_predict/lib/python3.8/site-packages (from mlflow) (0.15.0)\n",
            "Requirement already satisfied: entrypoints in /anaconda/envs/rinna_gpt2_predict/lib/python3.8/site-packages (from mlflow) (0.3)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /anaconda/envs/rinna_gpt2_predict/lib/python3.8/site-packages (from mlflow) (5.4.1)\n",
            "Requirement already satisfied: sqlparse>=0.3.1 in /anaconda/envs/rinna_gpt2_predict/lib/python3.8/site-packages (from mlflow) (0.4.1)\n",
            "Requirement already satisfied: sqlalchemy in /anaconda/envs/rinna_gpt2_predict/lib/python3.8/site-packages (from mlflow) (1.4.22)\n",
            "Requirement already satisfied: alembic<=1.4.1 in /anaconda/envs/rinna_gpt2_predict/lib/python3.8/site-packages (from mlflow) (1.4.1)\n",
            "Requirement already satisfied: protobuf>=3.7.0 in /anaconda/envs/rinna_gpt2_predict/lib/python3.8/site-packages (from mlflow) (3.17.3)\n",
            "Requirement already satisfied: python-editor>=0.3 in /anaconda/envs/rinna_gpt2_predict/lib/python3.8/site-packages (from alembic<=1.4.1->mlflow) (1.0.4)\n",
            "Requirement already satisfied: Mako in /anaconda/envs/rinna_gpt2_predict/lib/python3.8/site-packages (from alembic<=1.4.1->mlflow) (1.1.4)\n",
            "Requirement already satisfied: tabulate>=0.7.7 in /anaconda/envs/rinna_gpt2_predict/lib/python3.8/site-packages (from databricks-cli>=0.8.7->mlflow) (0.8.9)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /anaconda/envs/rinna_gpt2_predict/lib/python3.8/site-packages (from gitpython>=2.1.0->mlflow) (4.0.7)\n",
            "Requirement already satisfied: smmap<5,>=3.0.1 in /anaconda/envs/rinna_gpt2_predict/lib/python3.8/site-packages (from gitdb<5,>=4.0.1->gitpython>=2.1.0->mlflow) (4.0.0)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /anaconda/envs/rinna_gpt2_predict/lib/python3.8/site-packages (from sqlalchemy->mlflow) (1.1.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.2.1 in /anaconda/envs/rinna_gpt2_predict/lib/python3.8/site-packages (from onnx) (3.10.0.0)\n",
            "Requirement already satisfied: xmltodict in /anaconda/envs/rinna_gpt2_predict/lib/python3.8/site-packages (from py3nvml) (0.12.0)\n",
            "Requirement already satisfied: mpmath>=0.19 in /anaconda/envs/rinna_gpt2_predict/lib/python3.8/site-packages (from sympy) (1.2.1)\n",
            "Requirement already satisfied: backports.weakref in /anaconda/envs/rinna_gpt2_predict/lib/python3.8/site-packages (from backports.tempfile->azureml-core) (1.0.post1)\n",
            "Requirement already satisfied: itsdangerous>=2.0 in /anaconda/envs/rinna_gpt2_predict/lib/python3.8/site-packages (from Flask->mlflow) (2.0.1)\n",
            "Requirement already satisfied: Jinja2>=3.0 in /anaconda/envs/rinna_gpt2_predict/lib/python3.8/site-packages (from Flask->mlflow) (3.0.1)\n",
            "Requirement already satisfied: Werkzeug>=2.0 in /anaconda/envs/rinna_gpt2_predict/lib/python3.8/site-packages (from Flask->mlflow) (2.0.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /anaconda/envs/rinna_gpt2_predict/lib/python3.8/site-packages (from Jinja2>=3.0->Flask->mlflow) (2.0.1)\n",
            "Requirement already satisfied: setuptools>=3.0 in /anaconda/envs/rinna_gpt2_predict/lib/python3.8/site-packages (from gunicorn->mlflow) (52.0.0.post20210125)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /anaconda/envs/rinna_gpt2_predict/lib/python3.8/site-packages (from packaging->mlflow) (2.4.7)\n",
            "Requirement already satisfied: prometheus-client in /anaconda/envs/rinna_gpt2_predict/lib/python3.8/site-packages (from prometheus-flask-exporter->mlflow) (0.11.0)\n"
          ]
        }
      ],
      "metadata": {
        "gather": {
          "logged": 1628068194057
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "source": [
        "# キャッシュ保存用のディレクトリを用意\n",
        "import os\n",
        "cache_dir = os.path.join(\".\", \"cache_models\")\n",
        "if not os.path.exists(cache_dir):\n",
        "    os.makedirs(cache_dir)"
      ],
      "outputs": [],
      "metadata": {
        "gather": {
          "logged": 1628068194169
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "source": [
        "# Azure ML Workspace への接続\n",
        "from azureml.core import Experiment, Workspace, Environment\n",
        "from azureml.core.compute import ComputeTarget\n",
        "from azureml.core import ScriptRunConfig\n",
        "from azureml.core.runconfig import PyTorchConfiguration\n",
        "from azureml.core.authentication import InteractiveLoginAuthentication\n",
        "import mlflow\n",
        "\n",
        "interactive_auth = InteractiveLoginAuthentication(force=True,tenant_id=\"72f988bf-86f1-41af-91ab-2d7cd011db47\")\n",
        "ws = Workspace.from_config(path='config.json',auth=interactive_auth)\n",
        "\n",
        "mlflow.set_tracking_uri(ws.get_mlflow_tracking_uri())"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Performing interactive authentication. Please follow the instructions on the terminal.\n",
            "To sign in, use a web browser to open the page https://microsoft.com/devicelogin and enter the code HRSETETYG to authenticate.\n",
            "You have logged in. Now let us find all the subscriptions to which you have access...\n",
            "Interactive authentication successfully completed.\n"
          ]
        }
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## モデル読み込み"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "source": [
        "client = mlflow.tracking.MlflowClient()"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "source": [
        "registered_model = client.get_model_version(name='test-model',version=37)\n",
        "client.download_artifacts(registered_model.run_id, 'outputs/models', cache_dir)"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/home/shunta_ito/notebooks/mlops/nlp-samples/examples/rinna-gpt2-predict/cache_models/outputs/models'"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "source": [
        "# GPT-2 モデルにビームサーチを組み合わせるヘルパー class で読み込んだ GPT-2 モデルをラップ\n",
        "from onnxruntime.transformers.gpt2_beamsearch_helper import Gpt2BeamSearchHelper, GPT2LMHeadModel_BeamSearchStep\n",
        "from transformers import AutoConfig\n",
        "import torch\n",
        "\n",
        "model_name_or_path = os.path.join(cache_dir, 'outputs/models')\n",
        "config = AutoConfig.from_pretrained(model_name_or_path, cache_dir=model_name_or_path)\n",
        "model = GPT2LMHeadModel_BeamSearchStep.from_pretrained(model_name_or_path, config=config, batch_size=1, beam_size=4, cache_dir=cache_dir)\n",
        "device = torch.device(\"cpu\")\n",
        "model.eval().to(device)\n",
        "\n",
        "print(model.config)\n",
        "\n",
        "num_attention_heads = model.config.n_head\n",
        "hidden_size = model.config.n_embd\n",
        "num_layer = model.config.n_layer"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPT2Config {\n",
            "  \"_name_or_path\": \"./cache_models/outputs/models\",\n",
            "  \"activation_function\": \"gelu_new\",\n",
            "  \"architectures\": [\n",
            "    \"GPT2LMHeadModel\"\n",
            "  ],\n",
            "  \"attn_pdrop\": 0.1,\n",
            "  \"batch_size\": 1,\n",
            "  \"beam_size\": 4,\n",
            "  \"bos_token_id\": 1,\n",
            "  \"embd_pdrop\": 0.1,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"layer_norm_epsilon\": 1e-05,\n",
            "  \"model_type\": \"gpt2\",\n",
            "  \"n_ctx\": 1024,\n",
            "  \"n_embd\": 1024,\n",
            "  \"n_head\": 16,\n",
            "  \"n_inner\": 4096,\n",
            "  \"n_layer\": 24,\n",
            "  \"n_positions\": 1024,\n",
            "  \"resid_pdrop\": 0.1,\n",
            "  \"scale_attn_weights\": true,\n",
            "  \"summary_activation\": null,\n",
            "  \"summary_first_dropout\": 0.1,\n",
            "  \"summary_proj_to_labels\": true,\n",
            "  \"summary_type\": \"cls_index\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"task_specific_params\": {\n",
            "    \"text-generation\": {\n",
            "      \"do_sample\": true,\n",
            "      \"max_length\": 50\n",
            "    }\n",
            "  },\n",
            "  \"transformers_version\": \"4.8.2\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 32000\n",
            "}\n",
            "\n"
          ]
        }
      ],
      "metadata": {
        "gather": {
          "logged": 1628068248507
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## PyTorch GPT-2 モデルをビームサーチの1ステップを含む ONNX に変換 "
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "source": [
        "# ONNX に変換\n",
        "onnx_model_path = os.path.join(cache_dir, \"rinna_gpt2_beam_step_search.onnx\")\n",
        "\n",
        "if not os.path.exists(onnx_model_path):\n",
        "    Gpt2BeamSearchHelper.export_onnx(model, device, onnx_model_path) # add parameter use_external_data_format=True when model size > 2 GB\n",
        "else:\n",
        "    print(\"GPT-2 ONNX model exists.\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPT-2 ONNX model exists.\n"
          ]
        }
      ],
      "metadata": {
        "gather": {
          "logged": 1628068248683
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "source": [
        "# 混合精度と量子化\n",
        "from onnxruntime.transformers.gpt2_helper import Gpt2Helper, MyGPT2LMHeadModel\n",
        "from onnxruntime.transformers.quantize_helper import QuantizeHelper\n",
        "\n",
        "optimized_fp32_model_path = os.path.join(cache_dir, \"rinna_gpt2_beam_step_search_optimized.onnx\")\n",
        "quantized_int8_model_path = os.path.join(cache_dir, \"rinna_gpt2_beam_step_search_optimized_gpt2_int8.onnx\")\n",
        "\n",
        "if not os.path.exists(optimized_fp32_model_path):\n",
        "    Gpt2Helper.optimize_onnx(onnx_model_path, optimized_fp32_model_path, False, model.config.num_attention_heads, model.config.hidden_size)\n",
        "else:\n",
        "    print(\"Optimized GPT-2 FP32 ONNX model exists.\")\n",
        "\n",
        "if not os.path.exists(quantized_int8_model_path):   \n",
        "    QuantizeHelper.quantize_onnx_model(optimized_fp32_model_path, quantized_int8_model_path)\n",
        "else:\n",
        "    print(\"Quantized GPT-2 Int8 ONNX model exists.\")\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Optimized GPT-2 FP32 ONNX model exists.\n",
            "Quantized GPT-2 Int8 ONNX model exists.\n"
          ]
        }
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "source": [
        "# 量子化した ONNX をモデルとして登録\n",
        "mlflow.set_experiment('register_onnx')\n",
        "with mlflow.start_run() as run:\n",
        "    remote_model_path = os.path.join('outputs','onnx', \"rinna_gpt2_beam_step_search_optimized_gpt2_int8.onnx\")\n",
        "    mlflow.log_artifact(quantized_int8_model_path, remote_model_path)\n",
        "    model_uri = \"runs:/{}/\".format(run.info.run_id) + remote_model_path\n",
        "    mlflow.register_model(model_uri, 'rinna-GPT2-quantized-model')"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Registered model 'rinna-GPT2-quantized-model' already exists. Creating a new version of this model...\n",
            "2021/08/17 09:02:19 INFO mlflow.tracking._model_registry.client: Waiting up to 300 seconds for model version to finish creation.                     Model name: rinna-GPT2-quantized-model, version 1\n",
            "Created version '1' of model 'rinna-GPT2-quantized-model'.\n"
          ]
        }
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ONNX Runtime Inference ##\n",
        "\n",
        "We can use ONNX Runtime to inference. The inputs are dictionary with name and numpy array as value, and the output is list of numpy array. Note that both input and output are in CPU. When you run the inference in GPU, it will involve data copy between CPU and GPU for input and output.\n",
        "\n",
        "Let's create an inference session for ONNX Runtime given the exported ONNX model, and see the output."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "import onnxruntime\n",
        "import numpy\n",
        "from transformers import T5Tokenizer\n",
        "\n",
        "EXAMPLE_Text = ['私はりんなです。']\n",
        "\n",
        "def get_tokenizer(model_name_or_path, cache_dir):\n",
        "    tokenizer = T5Tokenizer.from_pretrained(model_name_or_path, cache_dir=cache_dir)\n",
        "    tokenizer.padding_side = \"left\"\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "    tokenizer.do_lower_case = True\n",
        "    #okenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
        "    return tokenizer\n",
        "\n",
        "def get_example_inputs(prompt_text=EXAMPLE_Text):    \n",
        "    tokenizer = get_tokenizer('rinna/japanese-gpt2-medium', cache_dir)\n",
        "    encodings_dict = tokenizer.batch_encode_plus(prompt_text, padding=True)\n",
        "\n",
        "    input_ids = torch.tensor(encodings_dict['input_ids'], dtype=torch.int64)\n",
        "    attention_mask = torch.tensor(encodings_dict['attention_mask'], dtype=torch.float32)\n",
        "    position_ids = (attention_mask.long().cumsum(-1) - 1)\n",
        "    position_ids.masked_fill_(position_ids < 0, 0)\n",
        "\n",
        "    #Empty Past State for generating first word\n",
        "    empty_past = []\n",
        "    batch_size = input_ids.size(0)\n",
        "    sequence_length = input_ids.size(1)\n",
        "    past_shape = [2, batch_size, num_attention_heads, 0, hidden_size // num_attention_heads]\n",
        "    for i in range(num_layer):\n",
        "        empty_past.append(torch.empty(past_shape).type(torch.float32).to(device))\n",
        "       \n",
        "    return input_ids, attention_mask, position_ids, empty_past\n",
        "\n",
        "input_ids, attention_mask, position_ids, empty_past = get_example_inputs()\n",
        "beam_select_idx = torch.zeros([1, input_ids.shape[0]]).long()\n",
        "input_log_probs = torch.zeros([input_ids.shape[0], 1])\n",
        "input_unfinished_sents = torch.ones([input_ids.shape[0], 1], dtype=torch.bool)\n",
        "prev_step_scores = torch.zeros([input_ids.shape[0], 1])\n",
        "\n",
        "session = onnxruntime.InferenceSession(onnx_model_path)\n",
        "ort_inputs = {\n",
        "              'input_ids': numpy.ascontiguousarray(input_ids.cpu().numpy()),\n",
        "              'attention_mask' : numpy.ascontiguousarray(attention_mask.cpu().numpy()),\n",
        "              'position_ids': numpy.ascontiguousarray(position_ids.cpu().numpy()),\n",
        "              'beam_select_idx': numpy.ascontiguousarray(beam_select_idx.cpu().numpy()),\n",
        "              'input_log_probs': numpy.ascontiguousarray(input_log_probs.cpu().numpy()),\n",
        "              'input_unfinished_sents': numpy.ascontiguousarray(input_unfinished_sents.cpu().numpy()),\n",
        "              'prev_step_results': numpy.ascontiguousarray(input_ids.cpu().numpy()),\n",
        "              'prev_step_scores': numpy.ascontiguousarray(prev_step_scores.cpu().numpy()),\n",
        "             }\n",
        "for i, past_i in enumerate(empty_past):\n",
        "    ort_inputs[f'past_{i}'] = numpy.ascontiguousarray(past_i.cpu().numpy())\n",
        "#print(ort_inputs)\n",
        "ort_outputs = session.run(None, ort_inputs)\n",
        "#print(ort_outputs)"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading:   0%|          | 0.00/806k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "adc30df306b84a798fe2395ddea43168"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading:   0%|          | 0.00/153 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "22f09208945a4847b08c625ebfc25c2b"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading:   0%|          | 0.00/282 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ea539b0b37ed406d9839c96258977ad4"
            }
          },
          "metadata": {}
        }
      ],
      "metadata": {
        "gather": {
          "logged": 1628068299895
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ONNX Runtime Inference with IO Binding ##\n",
        "\n",
        "To avoid data copy for input and output, ONNX Runtime also supports IO Binding. User could provide some buffer for input and outputs. For GPU inference, the buffer can be in GPU to reduce memory copy between CPU and GPU. This is helpful for high performance inference in GPU. For GPT-2, IO Binding might help the performance when batch size or (past) sequence length is large."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "def inference_with_io_binding(session, config, input_ids, position_ids, attention_mask, past, beam_select_idx, input_log_probs, input_unfinished_sents, prev_step_results, prev_step_scores, step, context_len):\n",
        "    output_shapes = Gpt2BeamSearchHelper.get_output_shapes(batch_size=1,\n",
        "                                                           context_len=context_len,\n",
        "                                                           past_sequence_length=past[0].size(3),\n",
        "                                                           sequence_length=input_ids.size(1),\n",
        "                                                           beam_size=4,\n",
        "                                                           step=step,\n",
        "                                                           config=config,\n",
        "                                                           model_class=\"GPT2LMHeadModel_BeamSearchStep\")\n",
        "    output_buffers = Gpt2BeamSearchHelper.get_output_buffers(output_shapes, device)\n",
        "\n",
        "    io_binding = Gpt2BeamSearchHelper.prepare_io_binding(session, input_ids, position_ids, attention_mask, past, output_buffers, output_shapes, beam_select_idx, input_log_probs, input_unfinished_sents, prev_step_results, prev_step_scores)\n",
        "    session.run_with_iobinding(io_binding)\n",
        "\n",
        "    outputs = Gpt2BeamSearchHelper.get_outputs_from_io_binding_buffer(session, output_buffers, output_shapes, return_numpy=False)\n",
        "    return outputs"
      ],
      "outputs": [],
      "metadata": {
        "gather": {
          "logged": 1628068300137
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can see that the result is exactly same with/without IO Binding:"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "input_ids, attention_mask, position_ids, empty_past = get_example_inputs()\n",
        "beam_select_idx = torch.zeros([1, input_ids.shape[0]]).long()\n",
        "input_log_probs = torch.zeros([input_ids.shape[0], 1])\n",
        "input_unfinished_sents = torch.ones([input_ids.shape[0], 1], dtype=torch.bool)\n",
        "prev_step_scores = torch.zeros([input_ids.shape[0], 1])\n",
        "outputs = inference_with_io_binding(session, config, input_ids, position_ids, attention_mask, empty_past, beam_select_idx, input_log_probs, input_unfinished_sents, input_ids, prev_step_scores, 0, input_ids.shape[-1])\n",
        "assert torch.eq(outputs[-2], torch.from_numpy(ort_outputs[-2])).all()\n",
        "print(\"IO Binding result is good\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "IO Binding result is good\n"
          ]
        }
      ],
      "metadata": {
        "gather": {
          "logged": 1628068303048
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Batch Text Generation ##\n",
        "\n",
        "Here is an example for text generation using ONNX Runtime with/without IO Binding."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "def update(output, step, batch_size, beam_size, context_length, prev_attention_mask, device):\n",
        "    \"\"\"\n",
        "    Update the inputs for next inference.\n",
        "    \"\"\"\n",
        "    last_state = (torch.from_numpy(output[0]).to(device)\n",
        "                        if isinstance(output[0], numpy.ndarray) else output[0].clone().detach().cpu())\n",
        "\n",
        "    input_ids = last_state.view(batch_size * beam_size, -1).to(device)\n",
        "\n",
        "    input_unfinished_sents_id = -3\n",
        "    prev_step_results = (torch.from_numpy(output[-2]).to(device) if isinstance(output[-2], numpy.ndarray)\n",
        "                                else output[-2].clone().detach().to(device))\n",
        "    position_ids = (torch.tensor([context_length + step - 1\n",
        "                                        ]).unsqueeze(0).repeat(batch_size * beam_size, 1).to(device))\n",
        "\n",
        "    if prev_attention_mask.shape[0] != (batch_size * beam_size):\n",
        "        prev_attention_mask = prev_attention_mask.repeat(batch_size * beam_size, 1)\n",
        "    attention_mask = torch.cat(\n",
        "        [\n",
        "            prev_attention_mask,\n",
        "            torch.ones([batch_size * beam_size, 1]).type_as(prev_attention_mask),\n",
        "        ],\n",
        "        1,\n",
        "    ).to(device)\n",
        "\n",
        "    beam_select_idx = (torch.from_numpy(output[input_unfinished_sents_id - 2]).to(device) if isinstance(\n",
        "        output[input_unfinished_sents_id - 2], numpy.ndarray) else output[input_unfinished_sents_id - 2].clone().detach().to(device))\n",
        "    input_log_probs = (torch.from_numpy(output[input_unfinished_sents_id - 1]).to(device) if isinstance(\n",
        "        output[input_unfinished_sents_id - 1], numpy.ndarray) else output[input_unfinished_sents_id - 1].clone().detach().to(device))\n",
        "    input_unfinished_sents = (torch.from_numpy(output[input_unfinished_sents_id]).to(device) if isinstance(\n",
        "        output[input_unfinished_sents_id], numpy.ndarray) else\n",
        "                                    output[input_unfinished_sents_id].clone().detach().to(device))\n",
        "    prev_step_scores = (torch.from_numpy(output[-1]).to(device)\n",
        "                                if isinstance(output[-1], numpy.ndarray) else output[-1].clone().detach().to(device))\n",
        "\n",
        "    past = []\n",
        "    if isinstance(output[1], tuple):  # past in torch output is tuple\n",
        "        past = list(output[1])\n",
        "    else:\n",
        "        for i in range(model.config.n_layer):\n",
        "            past_i = (torch.from_numpy(output[i + 1])\n",
        "                        if isinstance(output[i + 1], numpy.ndarray) else output[i + 1].clone().detach())\n",
        "            past.append(past_i.to(device)) \n",
        "\n",
        "    inputs = {\n",
        "        'input_ids': input_ids,\n",
        "        'attention_mask' : attention_mask,\n",
        "        'position_ids': position_ids,\n",
        "        'beam_select_idx': beam_select_idx,\n",
        "        'input_log_probs': input_log_probs,\n",
        "        'input_unfinished_sents': input_unfinished_sents,\n",
        "        'prev_step_results': prev_step_results,\n",
        "        'prev_step_scores': prev_step_scores,\n",
        "    }\n",
        "    ort_inputs = {\n",
        "        'input_ids': numpy.ascontiguousarray(input_ids.cpu().numpy()),\n",
        "        'attention_mask' : numpy.ascontiguousarray(attention_mask.cpu().numpy()),\n",
        "        'position_ids': numpy.ascontiguousarray(position_ids.cpu().numpy()),\n",
        "        'beam_select_idx': numpy.ascontiguousarray(beam_select_idx.cpu().numpy()),\n",
        "        'input_log_probs': numpy.ascontiguousarray(input_log_probs.cpu().numpy()),\n",
        "        'input_unfinished_sents': numpy.ascontiguousarray(input_unfinished_sents.cpu().numpy()),\n",
        "        'prev_step_results': numpy.ascontiguousarray(prev_step_results.cpu().numpy()),\n",
        "        'prev_step_scores': numpy.ascontiguousarray(prev_step_scores.cpu().numpy()),\n",
        "    }\n",
        "    for i, past_i in enumerate(past):\n",
        "        ort_inputs[f'past_{i}'] = numpy.ascontiguousarray(past_i.cpu().numpy())\n",
        "    \n",
        "    return inputs, ort_inputs, past\n",
        "\n",
        "def test_generation(tokenizer, input_text, use_onnxruntime_io, ort_session = None, num_tokens_to_produce = 30):\n",
        "    print(\"Text generation using\", \"OnnxRuntime with IO binding\" if use_onnxruntime_io else \"OnnxRuntime\", \"...\")    \n",
        "    input_ids, attention_mask, position_ids, past = get_example_inputs(input_text)\n",
        "    beam_select_idx = torch.zeros([1, input_ids.shape[0]]).long()\n",
        "    input_log_probs = torch.zeros([input_ids.shape[0], 1])\n",
        "    input_unfinished_sents = torch.ones([input_ids.shape[0], 1], dtype=torch.bool)\n",
        "    prev_step_scores = torch.zeros([input_ids.shape[0], 1])\n",
        "    inputs = {\n",
        "        'input_ids': input_ids,\n",
        "        'attention_mask' : attention_mask,\n",
        "        'position_ids': position_ids,\n",
        "        'beam_select_idx': beam_select_idx,\n",
        "        'input_log_probs': input_log_probs,\n",
        "        'input_unfinished_sents': input_unfinished_sents,\n",
        "        'prev_step_results': input_ids,\n",
        "        'prev_step_scores': prev_step_scores,\n",
        "    }\n",
        "    ort_inputs = {\n",
        "        'input_ids': numpy.ascontiguousarray(input_ids.cpu().numpy()),\n",
        "        'attention_mask' : numpy.ascontiguousarray(attention_mask.cpu().numpy()),\n",
        "        'position_ids': numpy.ascontiguousarray(position_ids.cpu().numpy()),\n",
        "        'beam_select_idx': numpy.ascontiguousarray(beam_select_idx.cpu().numpy()),\n",
        "        'input_log_probs': numpy.ascontiguousarray(input_log_probs.cpu().numpy()),\n",
        "        'input_unfinished_sents': numpy.ascontiguousarray(input_unfinished_sents.cpu().numpy()),\n",
        "        'prev_step_results': numpy.ascontiguousarray(input_ids.cpu().numpy()),\n",
        "        'prev_step_scores': numpy.ascontiguousarray(prev_step_scores.cpu().numpy()),\n",
        "    }\n",
        "    for i, past_i in enumerate(past):\n",
        "        ort_inputs[f'past_{i}'] = numpy.ascontiguousarray(past_i.cpu().numpy())\n",
        "    batch_size = input_ids.size(0)\n",
        "    beam_size = 4\n",
        "    context_length = input_ids.size(-1)\n",
        "\n",
        "    for step in range(num_tokens_to_produce):\n",
        "        if use_onnxruntime_io:\n",
        "            outputs = inference_with_io_binding(ort_session, config, inputs['input_ids'], inputs['position_ids'], inputs['attention_mask'], past, inputs['beam_select_idx'], inputs['input_log_probs'], inputs['input_unfinished_sents'], inputs['prev_step_results'], inputs['prev_step_scores'], step, context_length)\n",
        "        else:\n",
        "            outputs = ort_session.run(None, ort_inputs) \n",
        "        inputs, ort_inputs, past = update(outputs, step, batch_size, beam_size, context_length, inputs['attention_mask'], device)\n",
        "\n",
        "        if not inputs['input_unfinished_sents'].any():\n",
        "            break\n",
        "\n",
        "    print(\"------------\")\n",
        "    print(tokenizer.decode(inputs['prev_step_results'][0], skip_special_tokens=True))"
      ],
      "outputs": [],
      "metadata": {
        "gather": {
          "logged": 1628068303255
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "import time\n",
        "input_text = EXAMPLE_Text"
      ],
      "outputs": [],
      "metadata": {
        "gather": {
          "logged": 1628068306335
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "start = time.time()\n",
        "test_generation(tokenizer, input_text, use_onnxruntime_io=False, ort_session=session)    \n",
        "elapsed_time = time.time() - start\n",
        "print (\"elapsed_time:{0}\".format(elapsed_time) + \"[sec]\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text generation using OnnxRuntime ...\n",
            "------------\n",
            "私はりんなです。 @audio_snowfox @audio_snowfox @audio_snowfox @au\n",
            "elapsed_time:4.960899353027344[sec]\n"
          ]
        }
      ],
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1628068311176
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, we use ONNX Runtime with IO binding to run again and we can see that the result is exactly same."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "start = time.time()\n",
        "test_generation(tokenizer, input_text, use_onnxruntime_io=True, ort_session=session)\n",
        "elapsed_time = time.time() - start\n",
        "print (\"elapsed_time:{0}\".format(elapsed_time) + \"[sec]\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text generation using OnnxRuntime with IO binding ...\n",
            "------------\n",
            "私はりんなです。 @audio_snowfox @audio_snowfox @audio_snowfox @au\n",
            "elapsed_time:5.494853973388672[sec]\n"
          ]
        }
      ],
      "metadata": {
        "gather": {
          "logged": 1628068316843
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Quantatize GPT-2"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "session_int8 = onnxruntime.InferenceSession(quantized_int8_model_path)\n",
        "\n",
        "start = time.time()\n",
        "test_generation(tokenizer, input_text, use_onnxruntime_io=False, ort_session=session_int8)\n",
        "elapsed_time = time.time() - start\n",
        "print (\"elapsed_time:{0}\".format(elapsed_time) + \"[sec]\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text generation using OnnxRuntime ...\n",
            "------------\n",
            "私はりんなです。 @audio_snowfox @audio_snowfox @audio_snowfox @au\n",
            "elapsed_time:4.232302665710449[sec]\n"
          ]
        }
      ],
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1628068340468
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [],
      "outputs": [],
      "metadata": {}
    }
  ],
  "metadata": {
    "orig_nbformat": 2,
    "kernelspec": {
      "name": "rinna_gpt2_predict",
      "display_name": "rinna_gpt2_predict",
      "language": "python"
    },
    "metadata": {
      "interpreter": {
        "hash": "81098997110362167705b61d21e46dda767ff2050d805c22b6ba90fec7e1aa35"
      }
    },
    "kernel_info": {
      "name": "cpu_env"
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    },
    "microsoft": {
      "host": {
        "AzureML": {
          "notebookHasBeenCompleted": true
        }
      }
    },
    "interpreter": {
      "hash": "400a85a5d35626d27c756a250f88338f1168ed84ae0abbe7b1b61c0609847b83"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.11",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}