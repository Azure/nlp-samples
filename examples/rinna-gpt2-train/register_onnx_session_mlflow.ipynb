{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# InferenceSession を含む mlflow.pyfunc 対応の class\n",
        "import mlflow.pyfunc\n",
        "\n",
        "class ONNXrinnaGPT2(mlflow.pyfunc.PythonModel):\n",
        "    def __init__(self, gpt2_onnx_path, num_tokens_to_produce = 30, beam_size=4, use_onnxruntime_io=False, use_cpu=True):\n",
        "        import os\n",
        "        import torch\n",
        "        self.cache_dir = os.path.join(\".\", \"cache_models\")\n",
        "        self.onnx_bytes = self.get_onnx_bytes(gpt2_onnx_path)\n",
        "        #self.ort_session = onnxruntime.InferenceSession(gpt2_onnx_path) \n",
        "        # self で InferenceSession を持つと mlflow.pyfunc.save_model の pickle でこける\n",
        "        # https://github.com/microsoft/onnxruntime/issues/643\n",
        "        # https://github.com/microsoft/onnxruntime/pull/800\n",
        "        self.model_name_or_path = \"rinna/japanese-gpt2-medium\"\n",
        "        self.tokenizer = self.get_tokenizer()\n",
        "        self.num_layer = 24\n",
        "        self.num_attention_heads = 16\n",
        "        self.hidden_size = 1024\n",
        "\n",
        "        self.num_tokens_to_produce = num_tokens_to_produce\n",
        "        self.beam_size = beam_size\n",
        "        self.use_onnxruntime_io = use_onnxruntime_io\n",
        "        if use_cpu:\n",
        "            self.device =  torch.device(\"cpu\")\n",
        "        else:    \n",
        "            self.device =  torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    def get_onnx_bytes(self, path):\n",
        "        with open(path, 'rb') as f:\n",
        "            return f.read()\n",
        "        \n",
        "    def get_tokenizer(self):\n",
        "        from transformers import T5Tokenizer\n",
        "        if not os.path.exists(self.cache_dir):\n",
        "            os.makedirs(self.cache_dir)\n",
        "        tokenizer = T5Tokenizer.from_pretrained(self.model_name_or_path, cache_dir=self.cache_dir)\n",
        "        tokenizer.padding_side = \"left\"\n",
        "        tokenizer.pad_token = tokenizer.eos_token\n",
        "        tokenizer.do_lower_case = True\n",
        "        #okenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
        "        return tokenizer\n",
        "\n",
        "    def get_example_inputs(self, prompt_text):\n",
        "        import torch \n",
        "        encodings_dict = self.tokenizer.batch_encode_plus(prompt_text, padding=True)\n",
        "\n",
        "        input_ids = torch.tensor(encodings_dict['input_ids'], dtype=torch.int64)\n",
        "        attention_mask = torch.tensor(encodings_dict['attention_mask'], dtype=torch.float32)\n",
        "        position_ids = (attention_mask.long().cumsum(-1) - 1)\n",
        "        position_ids.masked_fill_(position_ids < 0, 0)\n",
        "\n",
        "        #Empty Past State for generating first word\n",
        "        empty_past = []\n",
        "        batch_size = input_ids.size(0)\n",
        "        sequence_length = input_ids.size(1)\n",
        "        past_shape = [2, batch_size, self.num_attention_heads, 0, self.hidden_size // self.num_attention_heads]\n",
        "        for i in range(self.num_layer):\n",
        "            empty_past.append(torch.empty(past_shape).type(torch.float32).to(self.device))\n",
        "\n",
        "        return input_ids, attention_mask, position_ids, empty_past\n",
        "\n",
        "    def inference_with_io_binding(self, session, config, input_ids, position_ids, attention_mask, past, beam_select_idx, input_log_probs, input_unfinished_sents, prev_step_results, prev_step_scores, step, context_len):\n",
        "        from onnxruntime.transformers.gpt2_beamsearch_helper import Gpt2BeamSearchHelper, GPT2LMHeadModel_BeamSearchStep\n",
        "        output_shapes = Gpt2BeamSearchHelper.get_output_shapes(batch_size=1,\n",
        "                                                               context_len=context_len,\n",
        "                                                               past_sequence_length=past[0].size(3),\n",
        "                                                               sequence_length=input_ids.size(1),\n",
        "                                                               beam_size=self.beam_size,\n",
        "                                                               step=step,\n",
        "                                                               config=config,\n",
        "                                                               model_class=\"GPT2LMHeadModel_BeamSearchStep\")\n",
        "        output_buffers = Gpt2BeamSearchHelper.get_output_buffers(output_shapes, self.device)\n",
        "\n",
        "        io_binding = Gpt2BeamSearchHelper.prepare_io_binding(session, input_ids, position_ids, attention_mask, past, output_buffers, output_shapes, beam_select_idx, input_log_probs, input_unfinished_sents, prev_step_results, prev_step_scores)\n",
        "        session.run_with_iobinding(io_binding)\n",
        "\n",
        "        outputs = Gpt2BeamSearchHelper.get_outputs_from_io_binding_buffer(session, output_buffers, output_shapes, return_numpy=False)\n",
        "        return outputs\n",
        "\n",
        "\n",
        "    def update(self, output, step, batch_size, beam_size, context_length, prev_attention_mask, device):\n",
        "        \"\"\"\n",
        "        Update the inputs for next inference.\n",
        "        \"\"\"\n",
        "        import numpy\n",
        "        import torch\n",
        "\n",
        "        last_state = (torch.from_numpy(output[0]).to(device)\n",
        "                            if isinstance(output[0], numpy.ndarray) else output[0].clone().detach().cpu())\n",
        "\n",
        "        input_ids = last_state.view(batch_size * beam_size, -1).to(device)\n",
        "\n",
        "        input_unfinished_sents_id = -3\n",
        "        prev_step_results = (torch.from_numpy(output[-2]).to(device) if isinstance(output[-2], numpy.ndarray)\n",
        "                                    else output[-2].clone().detach().to(device))\n",
        "        position_ids = (torch.tensor([context_length + step - 1\n",
        "                                            ]).unsqueeze(0).repeat(batch_size * beam_size, 1).to(device))\n",
        "\n",
        "        if prev_attention_mask.shape[0] != (batch_size * beam_size):\n",
        "            prev_attention_mask = prev_attention_mask.repeat(batch_size * beam_size, 1)\n",
        "        attention_mask = torch.cat(\n",
        "            [\n",
        "                prev_attention_mask,\n",
        "                torch.ones([batch_size * beam_size, 1]).type_as(prev_attention_mask),\n",
        "            ],\n",
        "            1,\n",
        "        ).to(device)\n",
        "\n",
        "        beam_select_idx = (torch.from_numpy(output[input_unfinished_sents_id - 2]).to(device) if isinstance(\n",
        "            output[input_unfinished_sents_id - 2], numpy.ndarray) else output[input_unfinished_sents_id - 2].clone().detach().to(device))\n",
        "        input_log_probs = (torch.from_numpy(output[input_unfinished_sents_id - 1]).to(device) if isinstance(\n",
        "            output[input_unfinished_sents_id - 1], numpy.ndarray) else output[input_unfinished_sents_id - 1].clone().detach().to(device))\n",
        "        input_unfinished_sents = (torch.from_numpy(output[input_unfinished_sents_id]).to(device) if isinstance(\n",
        "            output[input_unfinished_sents_id], numpy.ndarray) else\n",
        "                                        output[input_unfinished_sents_id].clone().detach().to(device))\n",
        "        prev_step_scores = (torch.from_numpy(output[-1]).to(device)\n",
        "                                    if isinstance(output[-1], numpy.ndarray) else output[-1].clone().detach().to(device))\n",
        "\n",
        "        past = []\n",
        "        if isinstance(output[1], tuple):  # past in torch output is tuple\n",
        "            past = list(output[1])\n",
        "        else:\n",
        "            for i in range(self.num_layer):\n",
        "                past_i = (torch.from_numpy(output[i + 1])\n",
        "                            if isinstance(output[i + 1], numpy.ndarray) else output[i + 1].clone().detach())\n",
        "                past.append(past_i.to(device)) \n",
        "\n",
        "        inputs = {\n",
        "            'input_ids': input_ids,\n",
        "            'attention_mask' : attention_mask,\n",
        "            'position_ids': position_ids,\n",
        "            'beam_select_idx': beam_select_idx,\n",
        "            'input_log_probs': input_log_probs,\n",
        "            'input_unfinished_sents': input_unfinished_sents,\n",
        "            'prev_step_results': prev_step_results,\n",
        "            'prev_step_scores': prev_step_scores,\n",
        "        }\n",
        "        ort_inputs = {\n",
        "            'input_ids': numpy.ascontiguousarray(input_ids.cpu().numpy()),\n",
        "            'attention_mask' : numpy.ascontiguousarray(attention_mask.cpu().numpy()),\n",
        "            'position_ids': numpy.ascontiguousarray(position_ids.cpu().numpy()),\n",
        "            'beam_select_idx': numpy.ascontiguousarray(beam_select_idx.cpu().numpy()),\n",
        "            'input_log_probs': numpy.ascontiguousarray(input_log_probs.cpu().numpy()),\n",
        "            'input_unfinished_sents': numpy.ascontiguousarray(input_unfinished_sents.cpu().numpy()),\n",
        "            'prev_step_results': numpy.ascontiguousarray(prev_step_results.cpu().numpy()),\n",
        "            'prev_step_scores': numpy.ascontiguousarray(prev_step_scores.cpu().numpy()),\n",
        "        }\n",
        "        for i, past_i in enumerate(past):\n",
        "            ort_inputs[f'past_{i}'] = numpy.ascontiguousarray(past_i.cpu().numpy())\n",
        "    \n",
        "        return inputs, ort_inputs, past\n",
        "\n",
        "    def predict(self, input_text):\n",
        "        import numpy\n",
        "        import torch\n",
        "        import onnxruntime\n",
        "        print(\"Text generation using\", \"OnnxRuntime with IO binding\" if self.use_onnxruntime_io else \"OnnxRuntime\", \"...\")    \n",
        "        input_ids, attention_mask, position_ids, past = self.get_example_inputs([input_text])\n",
        "        beam_select_idx = torch.zeros([1, input_ids.shape[0]]).long()\n",
        "        input_log_probs = torch.zeros([input_ids.shape[0], 1])\n",
        "        input_unfinished_sents = torch.ones([input_ids.shape[0], 1], dtype=torch.bool)\n",
        "        prev_step_scores = torch.zeros([input_ids.shape[0], 1])\n",
        "        inputs = {\n",
        "            'input_ids': input_ids,\n",
        "            'attention_mask' : attention_mask,\n",
        "            'position_ids': position_ids,\n",
        "            'beam_select_idx': beam_select_idx,\n",
        "            'input_log_probs': input_log_probs,\n",
        "            'input_unfinished_sents': input_unfinished_sents,\n",
        "            'prev_step_results': input_ids,\n",
        "            'prev_step_scores': prev_step_scores,\n",
        "        }\n",
        "        ort_inputs = {\n",
        "            'input_ids': numpy.ascontiguousarray(input_ids.cpu().numpy()),\n",
        "            'attention_mask' : numpy.ascontiguousarray(attention_mask.cpu().numpy()),\n",
        "            'position_ids': numpy.ascontiguousarray(position_ids.cpu().numpy()),\n",
        "            'beam_select_idx': numpy.ascontiguousarray(beam_select_idx.cpu().numpy()),\n",
        "            'input_log_probs': numpy.ascontiguousarray(input_log_probs.cpu().numpy()),\n",
        "            'input_unfinished_sents': numpy.ascontiguousarray(input_unfinished_sents.cpu().numpy()),\n",
        "            'prev_step_results': numpy.ascontiguousarray(input_ids.cpu().numpy()),\n",
        "            'prev_step_scores': numpy.ascontiguousarray(prev_step_scores.cpu().numpy()),\n",
        "        }\n",
        "        for i, past_i in enumerate(past):\n",
        "            ort_inputs[f'past_{i}'] = numpy.ascontiguousarray(past_i.cpu().numpy())\n",
        "        batch_size = input_ids.size(0)\n",
        "        beam_size = self.beam_size\n",
        "        context_length = input_ids.size(-1)\n",
        "\n",
        "        ort_session = onnxruntime.InferenceSession(self.onnx_bytes)\n",
        "        # self で保持すると mlflow.pyfunc.save_model の cloudpickle でこけるため、都度セッションを作成\n",
        "        # self.ort_session を使用する場合は下記 ort_session -> self.ort_session に書き換え\n",
        "\n",
        "        for step in range(self.num_tokens_to_produce):\n",
        "            if self.use_onnxruntime_io:\n",
        "                outputs = self.inference_with_io_binding(ort_session, config, inputs['input_ids'], inputs['position_ids'], inputs['attention_mask'], past, inputs['beam_select_idx'], inputs['input_log_probs'], inputs['input_unfinished_sents'], inputs['prev_step_results'], inputs['prev_step_scores'], step, context_length)\n",
        "            else:\n",
        "                outputs = ort_session.run(None, ort_inputs) \n",
        "            inputs, ort_inputs, past = self.update(outputs, step, batch_size, beam_size, context_length, inputs['attention_mask'], self.device)\n",
        "\n",
        "            if not inputs['input_unfinished_sents'].any():\n",
        "                break\n",
        "\n",
        "        predict_sentences = [self.tokenizer.decode(candidate, skip_special_tokens=True) for candidate in inputs['prev_step_results']]\n",
        "        \n",
        "        return predict_sentences"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "model_path = \"rinna_gpt2_beam_step_search_optimized_gpt2_int8.onnx\"\n",
        "onnx_gpt2 = ONNXrinnaGPT2(model_path)"
      ],
      "outputs": [],
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1628133878193
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "onnx_gpt2.predict('私はりんな')"
      ],
      "outputs": [],
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1628133928679
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "import os\n",
        "from azureml.core import Workspace\n",
        "import mlflow\n",
        "\n",
        "ws = Workspace.from_config()\n",
        "mlflow.set_tracking_uri(ws.get_mlflow_tracking_uri())\n",
        "#mlflow.set_tracking_uri('http://localhost:5000')\n",
        "experiment_name = 'experiment_with_mlflow'\n",
        "mlflow.set_experiment(experiment_name)"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "conda_env = {\n",
        "    'name': 'mlflow-env',\n",
        "    'channels': ['conda-forge'],\n",
        "    'dependencies': [\n",
        "        'python=3.8.10',\n",
        "        'pip',\n",
        "        {'pip': [\n",
        "            'mlflow',\n",
        "            'cloudpickle==1.6.0',\n",
        "            'numpy',\n",
        "            'torch==1.9.0+cpu',\n",
        "            'torchvision==0.10.0+cpu',\n",
        "            'onnxruntime==1.8.1',\n",
        "            'sentencepiece',\n",
        "            'transformers==4.8.2',\n",
        "            'onnx',\n",
        "            'onnxconverter_common',\n",
        "            'psutil',\n",
        "            'pytz',\n",
        "            'pandas',\n",
        "            'py-cpuinfo',\n",
        "            'py3nvml',\n",
        "            'sympy', \n",
        "            'coloredlogs',\n",
        "            'azureml-core',\n",
        "            'azureml-mlflow'\n",
        "            ]\n",
        "        }\n",
        "    ]\n",
        "}"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "with mlflow.start_run() as run:\n",
        "    remote_quantized_int8_model_path = 'onnx/'+ quantized_int8_model_path\n",
        "    mlflow.log_artifact(quantized_int8_model_path, 'onnx/'+ quantized_int8_model_path)\n",
        "    model_uri = \"runs:/{}/\".format(run.info.run_id) + remote_quantized_int8_model_path # run_id を持っておけば with mlflow.start_run() as run は不要\n",
        "    #mlflow.register_model(model_uri, 'rinna-GPT2-quantized_int8-model')\n",
        "    mlflow.pyfunc.log_model(artifact_path='onnx/', python_model=onnx_gpt2, conda_env=conda_env,registered_model_name='onnx_beam')"
      ],
      "outputs": [],
      "metadata": {}
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python",
      "version": "3.8.10",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "orig_nbformat": 2,
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3.8.10 64-bit ('cpu_env': conda)"
    },
    "metadata": {
      "interpreter": {
        "hash": "81098997110362167705b61d21e46dda767ff2050d805c22b6ba90fec7e1aa35"
      }
    },
    "kernel_info": {
      "name": "cpu_env"
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    },
    "microsoft": {
      "host": {
        "AzureML": {
          "notebookHasBeenCompleted": true
        }
      }
    },
    "interpreter": {
      "hash": "e63b566899252a62e979e4a3d737cd16daa358113224295f0bf5a1840c8bc8ff"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}